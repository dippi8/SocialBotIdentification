% !TEX root = ../thesis.tex
\chapter{Features Engineering}
\label{capitolo4}
\thispagestyle{empty}

In this chapter we want to analyze the set of available features and expand it with new ones, based on the properties of tweets and pictures. 

\section{Baseline}
Some features are ready to be used in a classifier, while other ones need to be pre-processed to allow them to be more expressive. In this section we analyze the complete set of default profile features and which kind of pre-processing operation we applied. With this default set we trained a classifier to define a baseline and this allowed us to evaluate the improvements made by the features engineering step.

\small
\begin{center}
	\begin{tabular}{ccc}
		\\feature&type&preprocess operation\\
		\hline\hline
		id&int&delete\\
		name&str&delete - non-numeric feature\\
		screen\_name&str&delete - non-numeric feature\\
		statuses\_count&int&/\\
		followers\_count&int&/\\
		friends\_count&int&/\\
		favourites\_count&int&/\\
		listed\_count&int&/\\
		url&str&replace with hasUrl (0/1)\\
		lang&str&delete - non-numeric feature\\
		time\_zone&str&delete - too many missing values\\
		location&str&delete - too many missing values\\
		default\_profile&int&delete - too many missing values\\
		default\_profile\_image&boolean&boolean to int (0/1)\\
		geo\_enabled&boolean&delete - too many missing values\\
		profile\_image\_url&str&delete - non-numeric feature\\
		profile\_use\_background\_image&boolean&boolean to int (0/1)\\
		profile\_background\_image\_url\_https&str&delete - non-numeric feature\\
		profile\_text\_color&str&delete - non-numeric feature\\
		profile\_image\_url\_https&str&delete - non-numeric feature\\
		profile\_sidebar\_border\_color&str&delete - non-numeric feature\\
		profile\_background\_tile&boolean&boolean to int (0/1)\\
		profile\_sidebar\_fill\_color&str&delete - non-numeric feature\\
		profile\_background\_image\_url&str&delete - non-numeric feature\\
		profile\_background\_color&str&delete - non-numeric feature\\
		profile\_link\_color&str&delete - non-numeric feature\\
		utc\_offset&int&delete - too many missing values\\
		is\_translator&boolean&delete - too many missing values\\
		follow\_request\_sent&int&delete - relative feature\\
		protected&boolean&delete - too many missing values\\
		verified&boolean&delete - too many missing values\\
		notifications&boolean&delete - relative feature\\
		description&str&replace with hasDescription (0/1)\\
		contributors\_enabled&boolean&delete - too many missing values\\
		following&boolean&delete - relative feature\\
		created\_at&str&delete\\\hline\\
	\end{tabular}
\end{center}
\normalsize

Features processed as "delete - relative feature" are those ones related to the user who performed the scraping. So we didn't need them.

\section{Missing values filling}
\section{Descriptive features}
The feature engineering process started with this idea: we aimed to enhance our data with informations driven by the tweets that our users made.\\
The choice over the amount of tweets that would have been considered was a trade-off between performance and prediction speed.\\
We finally chose to retrive up to the latest 100 tweets for each user, because further material led us to a slower, but equivalent, prediction over test samples.
In order to enrich our attributes and to provide support to our algorithms, we decided to add some decriptive "meta" features, such as synthesis statistics and counters.\\
Their purpose is to describe the tweets in a statistical way, adding ranges and means to the attributes provided by the official APIs.\\
Each of these new values were been added to the users feature vector, in order to append new informations for each account.
Here is the list of this first 18 brand new features, introduced by our work:

\small
\begin{center}
	\begin{tabular}{ccc}
		\\feature&description\\
		\hline\hline
		avg\_len&average lenght of the tweets (words)\\
		max\_len&lenght of the longest tweet (words)\\
		min\_len&lenght of the shortest tweet (words)\\
		avg\_ret&average amount of retweets (by other users) per tweet\\
		max\_ret&highest amount of retweets (by other users) on a tweet\\
		min\_ret&lowest amount of retweets (by other users) on a tweet\\
		avg\_fav&average amount of favourites (by other users) per tweet\\
		max\_fav&highest amount of favourites (by other users) on a tweet\\
		min\_fav&lowest amount of favourites (by other users) on a tweet\\
		avg\_hash&average amount of hashtags involved in tweets\\
		max\_hash&highest amount of hashtags involved on a tweet\\
		min\_hash&highest amount of hashtags involved on a tweet\\
		freq&amount of tweets per day (up to 100)\\
		ret\_perc&percentage of retweets, made by the user, over its retrieved tweets\\
		media\_perc&percentage of media content incorpored in tweets\\
		url\_perc&percentage of URL links placed inside tweets\\
		quote\_perc&percentage of quotes, made by the user, over its retrieved tweets\\\hline\\
	\end{tabular}
\end{center}
\normalsize

\section{Intrinsict features}
Due the multiclass nature of our dataset, it was impossible to rely on the descriptive meta features only.\\
We faced the need of better capturing some behaviors, that could have helped us distinguish between targets.\\
We spent a lot of time analyzing Twitter timelines by ourselves. This was one of the most useful phases of our work.\\
Indeed, we have learnt a lot about bots acting like humans on the social platform.
One thing that was easy to notice was the monotony, in terms of words or URLs involved in tweets,  met with Spam-bots, as well as the opposites, for Genuine accounts or Fake-Followers.
We tried to encapsule this distinctive behavior by adding 2 intrinsic features to the training vector, along with the descriptive ones.\\
How to portray such monotony?\\
We though about different approaches, like complex sentiment analysis or entity recognition, but then, we chose to rely on a weighting technique and the euclidean distance.\\
We looked inside every retrieved tweets for each user, then we encoded each of them with TF-IDF weighting.\\
Every term (word) of every tweet was represented by a numeric weight, according to TF-IDF.\\
This weighting formula is a combination of Term Frequency (TF) and Inverse Document Frequency (IDF).
\[ TF_{i,j} =\frac {n_{i,j}}{|d_{j}|} \]
The term frequency factor counts the number \textit{n} of the \textit{i}\textsubscript{th} term inside the \textit{j}\textsubscript{th} document (the tweet, in our case), dividing it by the lenght of the latest, in order to give same importance to both short and long collections of texts.

\[ IDF_{i} =\log {\frac {|D|}{|\{d:i\in d\}|}} \]
Where \textit{d} is the document (tweet).
The invers document frequency factor aims to highlight the overall magnitude of the  i\textsubscript{th} term in the collection  which it belongs. The collection \textit{D}, in our work, is represented by all the gathered tweets of the examined user.
\[(TF-IDF)_{i} = TF_{i,j} \times IDF_{i} \]
After the encoding process, we wanted to map the resulting vectors into an euclidean space, in order to compute the distance of each weighted text, from the total centroid of the collection.\\
We decided to add each user a measure of the average intra-distance of its tweets.\\
In order to achieve this goal, we relied on the WSS metric used in K-means clustering, but trying to soften its magnitude. We didn't want huge ranges in our features, in order to minimize the normalizations along the process.\\
The resulting formula for this brand new attribute is the following:
\[IntraDistance(U) = \frac{1}{N}\sum _{\mathbf {x} \in U}\left\|\mathbf {x} -{\boldsymbol {\mu }}\right\|^{2}\]
Where \textit{N} is the number of tweet for user \textit{U}, \textit{x} is the encoded tweet and $\mu$ is the centroid of the tweet collection for that user.
This formula, as well as the whole process, has been used both for tweet words and URLs inside of them.
The resulting 2 features are
\small
\begin{center}
	\begin{tabular}{ccc}
		\\feature&type\\
		\hline\hline
		tweet\_intradistance&float\\
		url\_intradistance&float\\\hline\\
	\end{tabular}
\end{center}
\normalsize
We expected low values for Spam-bot accounts in both features, as well as we expected the opposite for Genuine and Fake-Follower accounts.


\section{Extrinsic features}
Once we have modeled the personal twitting actions, in terms of words and links dissimilarity, we needed to look for those parameters that could be compared with all the users in our dataset.
We wanted the users to get out of their shells, and to match their timelines with each others.\\
Once again, sentiment analysis came to our mind. We found lots of paid or limited services that could have only partially supported us during this stage.\\
We couldn't think about implementing our own semantic analysis, as it is meant to be, due to the effort and time it would had taken.\\
For semplicity sake, we had the idea to look for the most meaningful words in tweets, that are common to all the users belonging to the same class.\\
We tried this approach, hoping to find a robust help in separating topics among targets.\\
The idea was to build five not overlapping dictionaries, one for each class of users, containing the most popular words used by them in their retrieved tweets. Each dictionary is ordered according to the occurrence of each word, in order to have a proper ranking for the terms and to give each of them a score.
We gathered the 300 most common words for each category of accounts, in order to average about 250 terms, once the overlapping elements would have been discarded.
Considering more than 300 words led us to uninfluent perfromance boost, instead it required lot of time to process a single user, so we decided to stop early.\\
We have listed 5 dictionaries: 
\small
\begin{center}
	\begin{tabular}{ccc}
		\\dictionary&size\\
		\hline\hline
		NSFW\_main\_words.csv&209\\
		spam\_bots\_main\_words.csv&230\\
		news\_spreders\_main\_words.csv&237\\
		fake\_followers\_main\_words.csv&233\\
		genuine\_main\_words.csv&223\\\hline\\
	\end{tabular}
\end{center}
\normalsize
The overall scores are normalized, so that the most common word, for each class, is associated with a unitary weight, the least common one with a value similar to zero (it depends on the final amount of words included in the dictionary).\\
In terms of representation, this \textit{keywords score} is built with 5 different values, one for each class.\\
The dictionaries aren't totally disjointed with each others.
We decided to strip, from each list, only the top 50 words belonging to the other collections, since the most relevant scores reside in that zone of the ranking. We didn't want to give the user high scores, for the same word, but in different categories.\\
If some user hit a word that is placed inside more than a dictionary, we wanted to let the relative weights speak and assign scores to each of the targets that contain that word, knowing they are less relevant, as they don't fall in the highest positions.\\
So we have

\small
\begin{center}
	\begin{tabular}{ccc}
		\\feature&type\\
		\hline\hline
		NSFW\_words\_score&float\\
		spam\_bots\_words\_score&float\\
		news\_spreders\_words\_score&float\\
		fake\_followers\_words\_score&float\\
		genuine\_words\_score&float\\\hline\\
	\end{tabular}
\end{center}
\normalsize
When we process a new user and infer on him, we scan every words of every tweets and match them with all the dictionary. Every time that we hit a listed word, we assign the user the score of that word.\\
For instance, If the word we are comparing matches with the most occurrent for NSFW accounts (the top position in its dictionarye), we update the \textit{NSFW\_words\_score} of our user, summing 1 to its current value for that feature.
We expected to capture patterns about the choice of the words involved in tweets, for each of our categories.
This extrinsic attributes revealed themsevles as very useful, lately.


