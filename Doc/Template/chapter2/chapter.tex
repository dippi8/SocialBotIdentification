% !TEX root = ../thesis.tex
\chapter{State of the Art}
\label{capitolo2}
\thispagestyle{empty}

In telling bots and humans apart, lots of effort has been spent to find the most accurate detection system. In the past years, several interesting approaches have been tested to build bot classifiers, most of these strategies involved supervised machine learning algorithms, to highlight the hidden patterns characterizing automated behaviours. Although, some researches relied on other methods, such as graph-based inspections, human annotators and stacked models on different tiered data. All the aforementioned approaches shared the goal to better identify algorithmically-driven entities on social networks.

\section{Feature-based approaches}
The work that is most closely related to this thesis is\textit{ Botometer}, formerly known as BotOrNot \cite{Botometer, BotorNot}, an online tool that computes a bot-likelihood score for Twitter accounts and allows one to \emph{tell bots and genuine user accounts apart}. The tool builds on more 1000 features among network, user, friends, temporal, content and sentiment features, and uses a random forest classifier for each subset of features. The training data used is based on bot accounts collected in prior work by Lee et al. \cite{Lee11sevenmonths}, who used several Twitter honeypots to lure bots and collected about 36,000 candidate bot entities following or messaging their honeypot accounts. 
Botometer is a machine learning classifier system based on engineered features of heterogeneous nature. It considers attributes driven from sentiment analysis, timing, user's network, user content and meta-data. This feature-based approach led to an overall 0.94 score in AUC metric, measured with the Random Forest algorithm on a merged dataset, composed of the 36K aforementioned accounts and 3,000 new samples, manually annotated.
The goal of \textbf{BotBuster} is to build on the results of Botometer and not only to tell bots and humans apart, but also to distinguish different types of bots based on their potential of harming the people they interact with.

Cresci et al. \cite{Fake-Followers} specifically
focused on the problem of \emph{fake followers}. They constructed a dataset of human accounts (manually and by invitation of friends) and bought fake followers from online services like \url{http://fastfollowerz.com}. The work compares two types of classifiers, classifiers based on expert-defined rules and feature-based classifiers (machine learning), and shows (i) that fake followers 
can indeed be spotted and (ii) that black-box, feature-based classifiers perform better than white-box, rule-based classifiers. In addition, the work also produced a publicly available, labelled dataset that can be used for research purposes.

Other features-based studies took Botometer as a baseline, like the work did with \textit{Bot-Hunter} \cite{Bot-Hunter}. In terms of \emph{datasets} to be analysed in the bot detection process, Beskow and Carley \cite{Bot-Hunter} specifically propose four \emph{tiers} of data for the classification of Twitter accounts: single tweet text (tier 0), account + one tweet (1), account + full timeline (2), and account + timeline + friends timelines (3). They focused on the training set quality, recognizing the magnitude of this component inside a data science project, especially when data doesn't exist and must be collected with tricky methods. Their dataset was composed of 19,221 accounts labelled as bots, collected with event-based methods. The humans were gathered by the official APIs, retrieving \textit{normal} Twitter data. They collected 70,000 random users and sampled from them. Past researches \cite{Varol} asses that about 8\% of the Twitter population is automated. Beskow and Carley thought that the possible percentage of bots included in the human set was acceptable. With their tool Bot-Hunter, they however only study different machine learning techniques for tier-1, with performances comparable to similar models, as Botometer.

\section{Network-based approaches}
The problem of telling bots and humans apart has been investigated already before Botometer. For instance, Ratkiewicz et al. \cite{Astroturfing} studied the phenomenon of \emph{astroturfing}, i.e., political campaigns that aim to fake social support from people for a cause, and showed that bots play a major role in astroturfing activities in Twitter. This approach differs from the aforementioned ones, due to the graph-based structure involved to analyse the network. In order to represent the information flow on the social network, the researchers built a direct graph, in which nodes represent single accounts and edges stand for interaction, such as retweets or quotes. Edges are incrementally weighted, as new interactions between the same pair of users is observed. This research aimed to detect relevant memes through an automatic system and build some features for the observed memes. The system, named \textit{Truthy}, allows users to visualize memes and their related meta-data. As application of that system, the research led to a binary classifier built to label legitimate and truthy memes. The final model was chosen with the AdaBoost algorithm, 366 manually labelled memes, from which has been performed resampling, in order to balance the unbalanced dataset. The performance hit was 0.99 of AUC score.

Another network-based study is the one brought out by Cao et al. \cite{Sybils} describe \textit{SybilRank}, a tool for the detection of sybil accounts (bots) in social networks that analyzes the social graph to compute sybil-likelihood scores. Also differently from the other approached, the work studies the social graph of Facebook, not that of Twitter. The study highlights how, by that time, the feature-based approaches with machine learning models failed to be effective in social network intrusion detections \cite{Intrusion-Detection}. Cao et al. provided an useful tool of profiles ranking, based on the likelihood of an user to be a fake account, that can facilitate the manual verification of such users and the eventual suspension. It works on large scale, with a computational cost of $ O(n log(n) $).

A side project by the OSoMe (http://osome.iuni.iu.edu) researchers, which is also graph-based, is Hoaxy \cite{Misinformation, Hoaxy, Fact-Checking}.
Hoaxy is a web tool whose purpose is to show the spreading of misinformation through Twitter contents and spreaders. It performs queries via APIs and follows the graph of interactions flow, for a required topic or keyword. Interactions are meant as retweets, quotes and direct tweets. Every visualized spreader is given a colour, based on the Botometer score. A completed trusted profile si shown as a blue dot and a alleged bot is represented with a red circle. The diffusion network is also displayed as a graph, which has users on its nodes and interactions on its edges.


\section{Human-based approaches}

Most of the aforementioned works relied on a partial manual labelling process, in order to enhance their data and to asses the value of the automate classifiers. Cao et al. describe the human annotation ineffective, when it comes to large scales evaluations, due to the effort required for a single classification \cite{Sybils}.

The research made by Cresci et al. \cite{Cresci} highlighted the lacking of capability, by crowdsourcing annotators, to distinguish new waves of spambots from legitimate users. Indeed, the manual labelling of such bots has been performed with an accuracy that measured less than 24\%, with more than 1000 accounts misclassified as humans (false negatives). This experiment shows like some particular social bots are hard to detect, even by humans, making the automatic classification models more challenging to build.

Going beyond merely telling bots and humans apart, human-based methods are often used as ground-truth, like with the work done by Chu et al. \cite{Cyborgs}. They coined the term \emph{cyborg} to refer to bot-assisted humans in social networks and used a manually labelled dataset of 6000 randomly sampled Twitter accounts and a random forest classifier plus entropy measures to classify accounts into bots, cyborgs and humans.

Another finer-grained classification of bots is proposed by Varol et al. \cite{Varol}, who however propose a bottom-up approach to the identification of bots with similar online behaviour. The classifier used is the one adopted by Botometer, while the dataset used also included a manually annotated collection of Twitter accounts. After classifying users into bot or not, the authors further clustered the bot accounts into three types of bots: \emph{spammers}, \textit{self promoters}, and accounts that \textit{post content from applications}.

\section{Summary}
We learned that human-based approaches are much expensive, in terms of time and individuals involved, and often don't reach the desired performance, due to the quality of new waves of social bots.
Graph-based methods are good for large scale evaluations, using low computational complexities, but are usually build to get insights for moderators, as well as features for automatic classifiers.
We are beginning our work without a trusted multi-class dataset, it is reasonable to think that we won't need a graph-based structure to process our informations.
The feature-based methods seem to be the preferred to infer over bots and humans, even if the newborn content polluters are harder to detect, making the work more challenging.
We face the problem with an automatic classification system, based on engineered features.

In this thesis, we study different feature-based analysis techniques with the specific aim of distinguishing bots based on the potential harm they may cause in Twitter. The datasets we use can be understood as tier-2 datasets (account + full timeline of tweets), using the terminology by Beskow and Carley \cite{Bot-Hunter}. It is important to note that the work focuses on bots that interact with humans in online conversations; bots used for cyber attacks or for the automation of generic tasks are out of the scope. Our goal is to propose an online tool, as it was done with Botometer, that provides a deeper classification for the examined accounts. Acknowledging the works done before ours, we will start building the datasets following different leads and drawing by the available data that the previous researches collected. The methodology will be the same as the ones described in this chapter, sticking to the modus operandi for data science projects.
By studying the proposed papers, we learned that some models fit this kind of study better than others. We will try several classifiers and see if our needs will be fulfilled by the same algorithms seen in the state of art. 

We hope that this thesis will find its space among the discoveries found in telling bots and humans apart, adding new points of view in this research filed. The work we've done could become a baseline study, for the ones who will want to better characterize social bots, by distinguish their potential harming behaviours.
