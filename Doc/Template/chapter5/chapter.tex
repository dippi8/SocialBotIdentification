% !TEX root = ../thesis.tex
\chapter{Model selection}
\label{capitolo5}
\thispagestyle{empty}

In this chapter we will show the choices and stages behind the final model.
Starting from baseline models, we enhanced the chosen classifiers with handcrafted features coming from the last chapter.\\
We saw and studied the performance improvements with validation approaches, and this phase led us to our current solution.

The result involves three models:
\begin{itemize}
	\item[\PencilRight] a first Random Forest classifier that has been used to provide an early filter on the separation between genuine accounts and bots
	\item[\PencilRight] a second Random Forest that gives a classification among the four studied categories of bots only
	\item[\PencilRight] a Naive Bayes classifier, used over the same classes of the second Random Forest, but which reads and labels the users, according on their tweets only
	\item[\PencilRight] a K-neares-neighbors classifier, used over the four bot classes, based on the user's features only
\end{itemize}
The algorithm were used together into a pipeline workflow, whose first step is the detection of bots from humans, thanks to the first binary Random Forest.
Then, the percentage of membership in the bot category is further split into four sub-percentages, which represent the prediction over the inner bot categories.
This last partition is performed by a stacking ensemble, whose goal is to combine the predictions of the three multiclass models.

\
\section{Baselines}
The choices explained in this section were made at the same time of the ones listed in the Baseline section of the last chapter.

This is, basically, the same stage of the above-mentioned, but in a model-driven perspective.
The features involved are the ones described in section \ref{baseline}, but we started from that base, to try different classifiers over it.
We chose to evaluate the performances of raw classifiers, for both the binary and the mutliclass problem.
Each type of classifiers has been tested with the respective dataset, but considering only the baseline features of those data.

Furthermore, no parameters tuning has been applied, in order to minimize the results of our baselines classifier, with their standard settings.
\subsection{Random Forest}
Random forest is an ensemble learning method used in classification tasks and prediction ones as well.

The algorithm builds several \textit{decision trees} and the resulting output is provided by the mode of the predictions coming from the estimators in the forest.

Each decision tree is trained on a subset of the original data, formed by sampling with replacements the whole training set. They share the same splitting criterion, in order to build subtrees, which is the entropy:\\
Every tree computes the Information Gain of each feature, which is the difference, in terms of entropy, between the information gained on the data \textit{D}, before splitting on the attribute \textit{X}, and the one gained after the split, which provides \textit{n} subsets of \textit{D}.
\[{ \mathit{InformationGain(X)} = \mathit{Information(D)} - \mathit{Information_{X}(D)}}\]
where
\[{ \mathit{Information(D)} = - p_{1}\log p_{1} - ... - p_{n} \log p_{n}}\]
and
\[{ \mathit{Information_{X}(D)} = \frac{|D_{1}|}{|D|}Information(D_{1}) + ... + \frac{|D_{n}|}{|D|}Information(D_{n}) }\]
The attribute providing the highest InformationGain, against the others at the same level of the tree, is chosen to perform a split.

The feature set considered by each tree is a random subset of the original pool.

Due to its ability to face overfitting and to the feature importance ranking that it can provide, this tool is often preferred over other models belonging to the same category.

The advantage of preventing overfitting usually comes with a slower prediction time, because it needs enough estimators for this task.
But, for our purpose, there were enough estimators to face the variance problem without affecting the generalization speed.
\subsection{Logistic Regression}
Logistic regression is a common statistical model, that uses a sigmoid function to map the output of a linear regression on a normalized score, giving the probability, for each sample, to belong to the positive class, given its features and a weighting vector:
\[{\displaystyle P(\hat{y}_{i} = +1 | \vec{x}_{i}, \vec{w})={\frac {1}{1+e^{-\vec{w}h(\vec{x}_{i})}}}}\]

Where $ \hat{y}_{i} $ is the predicted target, over the  \textit{$i_{th}$} sample, \textit{$ \vec{x}_{i} $} is the feature vector of that sample, \textit{$ \vec{w} $} represents the weighting vector that has to be learned and \textit{h} is the activation function of the linear regression.

Logistic Regression searches for the weighting vector that matches the highest likelihood and, in order to do that, it minimizes a cross-entropy
error function, provided by the negative log of the likelihood:
\[{ \mathbf{L}(\vec{w}) = -\ln \prod\limits_{i=1}^{n} P(\hat{y}_{i} = +1 | \vec{x}_{i}, \vec{w})}\]

In multiclasses tasks, there are two possible approaches to face the problem:
\begin{itemize}
	\item[\PencilRight] a more general \textit{softmax} function to replace the logistic sigmoid, which assigns the probability, for the  \textit{$i_{th}$} sample, to belong to the class \textit{C}:
	\[{\displaystyle P(\mathbf{C}_{i} | \vec{x}_{i}, \vec{w})={\frac {e^{-\vec{w}h(\vec{x}_{i})}}{\sum\limits_{j = 1}^{n}e^{-\vec{w}h(\vec{x}_{j})}}}}\]
	\item[\PencilRight] "One-vs-Rest" method, which for each class, it builds a model that predicts the target class against all the others.
\end{itemize}
We decided to stick with the default settings of the libraries involved, so OvR was the approach used for the baseline.

\subsection{K-Nearest Neighbors}
K-Nearest Neighbors is an instance-based model used for classification, regression and pattern recognition. It is considered as a lazy learning algorithm, because all the computation is deferred until the prediction phase.
When it performs a classification over a new point, it looks for the \textit{K} nearest samples in the training set, according to a chosen metric, and it assigns, to the unseen sample, the mode of the targets of the retrieved neighbors.

The choices to make are the ones regarding the number \textit{K} of neighbors to consider, the weights to assign to them and the metric to calculate the distance with.
We used the default settings for the metric (\textit{Euclidean distance}) and for the weighting technique (\textit{uniform}), but we chose to consider 10 neighbors, because the automatic setting was \textit{K} = 5, which is the number of our possible targets.
We chose a \textit{K} that is large enough to make the model not too sensible to outliers, and restricted enough to sharpen the classes boundaries.

We first normalized the training data and then we fitted the algorithm on them, in order to simplify the distance computations.

\subsection{Support Vector Machine}
Support Vector Machine is a smart way to do instance-based learning. It can be seen as a generalization of the weighted KNN algorithm, with an arbitrary and feasible \textit{kernel function}, instead of the more generic dot product.

It can be summarised with a support vector $ \mathbf{\vec{x}} $ (a subset of the training set), a weighting vector $ \mathbf{\vec{w}} $ for them and a \textbf{kernel} \textit{K(x, x')} (a similarity function).

In order to make it work properly, three choices must be made:
\begin{itemize}
	\item[\PencilRight] a proper kernel, which is often selected according to experience and domain knowledge of the problem. We wanted to make things simple in this stage, so we used the default kernel function, which is the Radial Basis Function:\label{rbf}
	\[ K(x, x') = exp(- \frac{||x-x'||^{2}}{2\sigma^{2}}) \]
	with $ \sigma $ as a free parameter
	\item[\PencilRight] the weights $ \vec{w} $, which are obtained by maximizing the margin that splits the records belonging to different classes. Each samples are mapped into a space, thanks to what is known as the \textit{kernel trick}. The "trick" helps a linear classifier to work on a non-linear problem, applying the kernel function in the prediction phase.\\This process highlights the boundary that separates the points belonging to different classes.
	SVM aims to draw the boundary for the classes, in order to maximize the "margin" formed between the closest points that have different targets
	\item[\PencilRight] the support vector $ \vec{x} $, which comes as a consequence of choosing weights
\end{itemize}
Since we were still facing a multitarget problem, the binary nature of SVM must had been adapted to our needs. We decided, once again, to stick with the default setting for non-binary classifications, in order to have only raw baselines to compare.

The multitarget classification is handled with "One-vs-One" approach.
It considers all possible pairwise binary classifiers and so it leads to $\frac{N(N-1)}{2}$ individual binary classifiers, where N is the number of the classes in the problem.

In comparison with "One-vs-Rest" approach, "One-vs-One" is less sensitive to an imbalanced dataset, but it's more computationally expensive then the the other, which only builds N binary classifiers.
Despite our choices over methods and parameters weren't accurate in this stage as they were in the other ones, we decided to stick with this setting for SVM, because otherwise it would have led us to an irrelevant algorithm, in comparison with the above-mentioned.

\subsection{Comparison and baseline selection}
Different tasks imply different evaluation metrics. Every classifier was validated and selected according to certain indices of goodness. In particular, we followed a triple of metrics that involves Precision, Recall and F1, for the multiclass problem and we aimed to maximize AUC score for the binary case.
\subsubsection{Multiclass metric}
The selected baseline models were tested with a holdout approach at first, then with a crossvalidation method.
We built a Confusion Matrix for each model, in order to bring out goodness indices for each class, such as \textit{True Positive} (TP), \textit{False Positive} (FP) and \textit{False Negative} (FN).
The evaluation metrics considered are \textit{Precision}, \textit{Recall} and \textit{F1 score} and they work on the mentioned indices.
\begin{itemize}
	\item[\PencilRight] $ Precision = \frac{TP}{TP+FP} $\\
	It measures the proportion of positive identifications, for a given target, that was actually correct
	\item[\PencilRight] $ Recall = \frac{TP}{TP+FN} $\\
	It measures the proportion of actual positive classifications that was identified correctly
	\item[\PencilRight] $ F1 score = \frac{2(Precision \times Recall )}{Precision+Recall} $\\
	It calculates the harmonic mean of the previous metrics
\end{itemize}
Every metric is adapted to fit a multiclass problem. For each class, it has been computed this set of measures, and then they were averaged without weights (macro average), in order to not take label imbalance into account.

\subsubsection{Binary metric}
Since this classifier was built for a different purpose, with respect of the multiclass models, the \textit{Area Under the Curve} score (AUC) is the metric we followed, both for baselines and the final model evaluation.
Area Under the Curve represents the goodness of a classifier, in terms of the integral of the \textit{Receiver Operating Characteristic} (ROC curve), defined over the variation of a decision treshold.

The ROC curve lies in a bi-dimensional space, which has the \textit{True Positive Ratio} ($ TPR =  \frac{TP}{TP+FN}$) on the Y-axis, and the False Positive Ratio ($ FPR =  \frac{FP}{FP+TN}$ ) on the X-axis.
In general, a classifier should accomplish more than 0.5 in AUC score, because that threshold represents a random guesser, which has the 50\% of probabilities to detect the actual class.
The more the AUC score tends to 1, the better is the ability of the classifier to distinguish among classes.

The motivation behind the adding of this new metric is that we had a balanced binary dataset, and this metric is a good fit for this kind of problem. Moreover, Botmoter claims to have accomplished an AUC of 0.95, on a 10-fold crossvalidation test.
WE wanted to get close to that score, using our crafted features.


\subsection{Holdout evaluation}
The holdout stage is performed separating the samples in the dataset into training and test subsets. The splitting process is randomized and it requires a bigger portion of the original dataset to be inserted in the training data, comparing to the amount of samples that will form the test set. A common choice is to use a third of the data to evaluate the model.

\subsection{Multiclass}
In this case, we decided to use the 75\% of the data for the training set and the 25\% for the test set. This choice is a little bit different from the most common one, which builds the training set with 2/3 of the whole data, because we didn't dispose of a huge amount of records, so we preferred this ratio and then trying an other validation method for comparison.
Here we list the algorithm and their parameters, as they were written according to the Scikit-learn library for Python, their confusion matrix and their scores:
\begin{itemize}
	\item[\PencilRight] \textit{RandomForestClassifier(n\_estimators = 10, criterion = 'entropy')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cccc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{4}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{NSFW} & 
			\multicolumn{1}{c}{NS} &
			\multicolumn{1}{c}{SB} & 
			\multicolumn{1}{c}{FF} \\
			\cline{2-6}
			\multirow[c]{4}{*}{\rotatebox[origin=tr]{90}{Actual class}}
			& NSFW  & 1690 & 27 & 12 & 6\\
			& NS  & 27  & 785 & 30 &  0\\
			& SB  & 14  & 39 & 1280 & 5\\
			& FF  & 12 &  7 &  18 & 1215\\
			\cline{2-6}\\
		\end{tabular}\\
	}
	
	Precision: 0.957\\
	Recall: 0.958\\
	F1 score: 0.957

	\item[\PencilRight] \textit{LogisticRegression(fit\_intercept=True, max\_iter=100, penalty='l2')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cccc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{4}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{NSFW} & 
			\multicolumn{1}{c}{NS} &
			\multicolumn{1}{c}{SB} & 
			\multicolumn{1}{c}{FF} \\
			\cline{2-6}
			\multirow[c]{4}{*}{\rotatebox[origin=tr]{90}{Actual class}}
			& NSFW  & 1274 & 213 & 197 & 51\\
			& NS  & 24 & 740 & 75 & 3\\
			& SB  & 29 & 87 & 1144 & 78\\
			& FF  & 204 &  49 &  46 & 953\\
			\cline{2-6}\\
		\end{tabular}\\
	}
	
	Precision: 0.793\\
	Recall: 0.807\\
	F1 score: 0.794
	
	\item[\PencilRight] \textit{KNeighborsClassifier(n\_neighbors=10)}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cccc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{4}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{NSFW} & 
			\multicolumn{1}{c}{NS} &
			\multicolumn{1}{c}{SB} & 
			\multicolumn{1}{c}{FF} \\
			\cline{2-6}
			\multirow[c]{4}{*}{\rotatebox[origin=tr]{90}{Actual class}}
			& NSFW  & 1578 & 36 & 60 & 61\\
			& NS  & 81 & 685 & 63 & 13\\
			& SB  & 81 & 32 & 1187 & 38\\
			& FF  & 104 & 6 & 55 & 1087\\
			\cline{2-6}\\
		\end{tabular}\\
	}
	
	Precision: 0.883\\
	Recall: 0.869\\
	F1 score: 0.875
	
	\item[\PencilRight] \textit{SVC(kernel='rbf', decision\_function\_shape='ovo')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|ccc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{3}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{NSFW} & 
			\multicolumn{1}{c}{SB} & 
			\multicolumn{1}{c}{FF} \\
			\cline{2-5}
			\multirow[c]{3}{*}{\rotatebox[origin=tr]{90}{Actual class}}
			& NSFW  & 1735 & 0 & 0\\
			& NS  & 842 & 0 & 0\\
			& SB  & 1114 & 223 & 1\\
			& FF  & 483 & 0 & 769\\
			\cline{2-5}\\
		\end{tabular}\\
	}
	
	Precision: 0.603\\
	Recall: 0.445\\
	F1 score: 0.408
	
\end{itemize}

\subsubsection{Binary}
This evaluation was made with the common splitting ration between train and test set. Since we had 31,212 samples available, well balanced, we used two thirds (20,808) for the training set and the remaining (10,404) for the test set.
We wanted a first term of comparison, so, in the beginning, we evaluated the AUC metric with the holdout technique.
\begin{itemize}
	\item[\PencilRight] \textit{RandomForestClassifier(n\_estimators = 10, criterion = 'entropy')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{BOT} & 
			\multicolumn{1}{c}{GEN}  \\
			\cline{2-4}
			\multirow[c]{2}{*}{Actual class}
			& BOT  & 4314 & 336\\
			& GEN  & 554 & 4160\\
			\cline{2-4}
			\multicolumn{2}{r|}{AUC} & 
			\multicolumn{2}{l}{0.905}\\
		\end{tabular}\\
	}

	\item[\PencilRight] \textit{LogisticRegression(fit\_intercept=True, max\_iter=100, penalty='l2')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{BOT} & 
			\multicolumn{1}{c}{GEN}  \\
			\cline{2-4}
			\multirow[c]{2}{*}{Actual class}
			& BOT  & 3124 & 1526\\
			& GEN  & 558 & 4156\\
			\cline{2-4}
			\multicolumn{2}{r|}{AUC} & 
			\multicolumn{2}{l}{0.776}\\
		\end{tabular}\\
	}

	
	\item[\PencilRight] \textit{KNeighborsClassifier(n\_neighbors=10)}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{BOT} & 
			\multicolumn{1}{c}{GEN}  \\
			\cline{2-4}
			\multirow[c]{2}{*}{Actual class}
			& BOT  & 3698 &  952\\
			& GEN  & 1129 & 3585\\
			\cline{2-4}
			\multicolumn{2}{r|}{AUC} & 
			\multicolumn{2}{l}{0.777}\\
		\end{tabular}\\
	}

	
	\item[\PencilRight] \textit{SVC(kernel='rbf')}\\
	Confusion matrix:
	
	{
		\centering
		\begin{tabular}{@{}cc|cc@{}}
			\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
			\multicolumn{1}{c}{} & 
			\multicolumn{1}{c|}{} & 
			\multicolumn{1}{c}{BOT} & 
			\multicolumn{1}{c}{GEN}  \\
			\cline{2-4}
			\multirow[c]{2}{*}{Actual class}
			& BOT  & 4625 & 25\\
			& GEN  & 4676 & 38\\
			\cline{2-4}
			\multicolumn{2}{r|}{AUC} & 
			\multicolumn{2}{l}{0.501}\\
		\end{tabular}\\
	}

\end{itemize}

\subsection{Crossvalidation}
This approach is based on repeated holdouts. It is performed by splitting the whole data in \textit{K} non-overlapping folds, leading to \textit{K} different holdout evaluations. The results for each step are stored and the final evaluation is given by the mean of the \textit{K} evaluations. For each evaluation, one fold is used for testing, the other ones for training the models. A common practice is to set \textit{K = 10} and thus averaging 10 different evaluations.
This method is also known as \textit{K-fold crossvalidation}. We used a stratified approach, which takes care about keeping the labels balanced on each fold.

Due the need of performing ten steps, it is computationally more expensive then a simple holdout validation. In our case, it was feasible, in term of speed, because of the models complexity and the data amount. This situation held for both the binary and the mutliclass tasks.

The obtained scores are also more meaningful, with regards to holdout, because they are less sensitive to "lucky" or "unlucky" splits.

Here is the results for every baseline model:
\subsubsection{Multiclass}
\begin{itemize}
	\item[\PencilRight] \textit{RandomForestClassifier(n\_estimators = 10, criterion = 'entropy')}\\
	Mean precision: 0.947\\
	Mean recall: 0.945\\
	Mean f1 score: 0.943
	\item[\PencilRight]\textit{LogisticRegression(fit\_intercept=True, max\_iter=100, penalty='l2')}\\
	Mean precision: 0.827\\
	Mean recall: 0.815\\
	Mean f1 score: 0.815
	\item[\PencilRight]\textit{KNeighborsClassifier(n\_neighbors=10)}\\
	Mean precision: 0.878\\
	Mean recall: 0.858\\
	Mean f1 score: 0.862
	\item[\PencilRight]\textit{SVC(kernel='rbf', decision\_function\_shape='ovo')}\\
	Mean precision: 0.573\\
	Mean recall: 0.456\\
	Mean f1 score: 0.413
\end{itemize}

As the results show, the random forest algorithm is the one that achieves the best performances, even with default settings, on both holdout and 10-fold crossvalidation. We thus decided to consider it as the main tool to build our bot categories classifier. 
\subsubsection{Binary}
\begin{itemize}
	\item[\PencilRight] \textit{RandomForestClassifier(n\_estimators = 10, criterion = 'entropy')}\\
	Mean AUC: 0.916\\
	\item[\PencilRight]\textit{LogisticRegression(fit\_intercept=True, max\_iter=100, penalty='l2')}\\
	Mean AUC: 0.792\\
	\item[\PencilRight]\textit{KNeighborsClassifier(n\_neighbors=10)}\\
	Mean AUC: 0.835\\
	Mean precision: 0.779\\
	\item[\PencilRight]\textit{SVC(kernel='rbf')}\\
	Mean AUC: 0.583\\
\end{itemize}
Even in the binary cases, the Random Forest had the best performance, and it could be imputed to the similar features involved in both problems. Moreover, we could see that the Support Vector Machine emerged as a lightly improved random guesser.


\section{Binary Classifier}
Since our dataset was pretty balanced and we couldn't retrieve much more genuine accounts, we didn't want our instrument to treat this category of users just as one the other bot kinds. It was important to perform a previous filter that was able to give importance to the separation between bots and genuine accounts.

We was inspired by the work made with Botometer\cite{Botometer}, which involved a binary labelled dataset, with bot and genuine accounts.
They built their features, grouped them in six main categories, then they ran a Random Forest algorithm per group.

We already had our feature engineering done, so we decided to test it on this new task.

In order to not to build a poorer version of our multiclass model, we didn't want to use a reduced copy of our dataset, stratifying it by stripping random bots from it. We needed a balanced dataset, with about the same amount of genuines and bots. So, we started from the same dataset used by the Botometer project, in order to have a baseline comparison.
\subsection{Dataset}
The dataset we used for this classification was composed by part of our collected records and by some entries from the Carvelee-2011 dataset, which contains 22,223 content polluters and 19,276 legitimate users, both collected through a social honeypot, as described in their paper\cite{Lee11sevenmonths}.

We setted the APIs to retrieve the ids for both genuines and bots, from the Carvelee list. The process provided us 15,687 legitimate user ids, and 15,525 general bot ids (without inner classifications), for a total number of 31,212 samples.
The difference from the original number of entries is due to the age of the dataset. Since 2011, the year of the creation of the list, a lot of accounts have been deleted or suspended.

The feature vector we used is the same that came out from the feature engineering process\ref{sec:feature_vector}, except for the specific characterizing features, that weren't considered, because crafted for the inner separation among bots. We excluded the \textit{NSFW\_avg}) image feature, as we noticed it didn't bring much performance boosting with the multiclass models.
The extrinsic features must had been adjusted with new dictionaries, so we had two features: (\textit{bots\_words\_score} and  \textit{genuine\_words\_score}.
Both the features have been computed as for the multiclass case, with up to 1000 non overlapping words in each dictionary.

\subsection{Model}
The model chosen for the purpose was the best performer of the tested baselines: the Random Forest binary classifier.
The algorithm has had its parameters tuned during the validation phase.
We decided to stick with 10-fold crossvalidation, as it was done for the baselines.

After several Grid Search runs, the last round computed had this hyperparameters to combine together:
\begin{itemize}
	\item[\PencilRight] \textit{n\_estimators} = [150, 200, 250, 300, 350, 400, 450, 500]
	\item[\PencilRight]\textit{max\_depth} = [None, 26, 28]
	\item[\PencilRight]\textit{criterion} = 'entropy'
\end{itemize}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/bon_tuning.png}
	\caption{Grid search results}
	\label{fig:grid_search}
\end{figure}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/auc.png}
	\caption{ROC curve}
	\label{fig:auc}
\end{figure}
As we can see in Figure \ref{fig:grid_search}, the AUC is increasing with the number of the estimators in the forest. We decided t stop at 450, which corresponds to the highest AUC score, since this phase was aimed to find a comparison term with Botometer, but it didn't represent the final model.
In their paper \cite{Varol}, the Botometer group claims to reach 0.95 in AUC score.

The AUC obtained with our arrangement is equal to 0.96, as shown in Figure \ref{fig:auc}, which is a positive accomplishment, considering that it will be used only as support for the identification of humans among bots, but we didn't crafted specific features as the ones involved in the Botmoter project and we didn't have the same amount of data neither.

The model has then been fitted with the hole data, with this settings: \textit{ n\_estimators} = 450, \textit{max\_depth} = 26 and \textit{criterion} = 'entropy'.

\subsection{Validation}

We had an interesting amount of data that were not involved in this task, because of the comparison with the same Botometer's dataset. Since this unseen data had a further discrimination among bots, it was easy to sample some records randomly, replacing their multiclass targets with binary values.
We performed this job to validate the newborn model on unseen and fresher data.
We were interested in testing a model that were trained over "old" accounts, with consequent different attributes values and different behaviours on the platform, with younger accounts.

This validations would had given us a preview of the real performance of the model, once it would had been deployed on the internet. The account that a user would test with our application could be younger than the ones included in the Carvelee's list.

We sampled 6,000 accounts, divided in 3,000 genuine and 3,000 bot ids, randomly picked by our multiclass dataset.

The binary model were fitted with its data and it was ready to perform new predictions.

Looking at the most relevant features for the classifier, as shown in Figure \ref{fig:bon_importances}, we could find the \textbf{age} field at the top position.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/bon_importances.png}
	\caption{Binary Random Forest features ranking}
	\label{fig:bon_importances}
\end{figure}

This was the first warning of a validation performance worsening.
A said before, the age of the accounts in the Carvelee's dataset were higher than the ones in our dataset. In particular, we examined the \textit{age} field of the training set, and the one coming from our validation samples, picked from the mutliclass dataset.
\begin{table}[!htb]
	\caption{Age field comparison among bot accounts}
	\begin{center}
		\begin{tabular}{@{}lr@{}}
			\multicolumn{2}{c}{\textbf{Training Bots}}\\
			\hline\hline
			\multicolumn{2}{c}{\textit{age}}\\
			\hline
			\multicolumn{1}{l}{mean}& \multicolumn{1}{r}{8}\\
			\multicolumn{1}{l}{std}& \multicolumn{1}{r}{0.66}\\
			\multicolumn{1}{l}{min}& \multicolumn{1}{r}{4}\\
			\multicolumn{1}{l}{max}& \multicolumn{1}{r}{12}\\
			\multicolumn{1}{l}{25\%}& \multicolumn{1}{r}{8}\\
			\multicolumn{1}{l}{50\%}& \multicolumn{1}{r}{9}\\
			\multicolumn{1}{l}{75\%}& \multicolumn{1}{r}{9}\\
			\hline\hline
		\end{tabular}
		\begin{tabular}{@{}lr@{}}
			\multicolumn{2}{c}{\textbf{Validation Bots}}\\
			\hline\hline
			\multicolumn{2}{c}{\textit{age}}\\
			\hline
			\multicolumn{1}{l}{mean}& \multicolumn{1}{r}{4.50}\\
			\multicolumn{1}{l}{std}& \multicolumn{1}{r}{2.69}\\
			\multicolumn{1}{l}{min}& \multicolumn{1}{r}{0}\\
			\multicolumn{1}{l}{max}& \multicolumn{1}{r}{11}\\
			\multicolumn{1}{l}{25\%}& \multicolumn{1}{r}{3}\\
			\multicolumn{1}{l}{50\%}& \multicolumn{1}{r}{4}\\
			\multicolumn{1}{l}{75\%}& \multicolumn{1}{r}{6}\\
			\hline\hline
		\end{tabular}
	\end{center}
	\label{table:bots}
\end{table}


\begin{table}[!htb]
	\caption{Age field comparison among genuine accounts}
	\begin{center}
		\begin{tabular}{@{}lr@{}}
			\multicolumn{2}{c}{\textbf{Training Genuine}}\\
			\hline\hline
			\multicolumn{2}{c}{\textit{age}}\\
			\hline
			\multicolumn{1}{l}{mean}& \multicolumn{1}{r}{9.22}\\
			\multicolumn{1}{l}{std}& \multicolumn{1}{r}{0.54}\\
			\multicolumn{1}{l}{min}& \multicolumn{1}{r}{4}\\
			\multicolumn{1}{l}{max}& \multicolumn{1}{r}{12}\\
			\multicolumn{1}{l}{25\%}& \multicolumn{1}{r}{9}\\
			\multicolumn{1}{l}{50\%}& \multicolumn{1}{r}{9}\\
			\multicolumn{1}{l}{75\%}& \multicolumn{1}{r}{9}\\
			\hline\hline
		\end{tabular}
		\begin{tabular}{@{}lr@{}}
			\multicolumn{2}{c}{\textbf{Validation Genuine}}\\
			\hline\hline
			\multicolumn{2}{c}{\textit{age}}\\
			\hline
			\multicolumn{1}{l}{mean}& \multicolumn{1}{r}{6.40}\\
			\multicolumn{1}{l}{std}& \multicolumn{1}{r}{1.94}\\
			\multicolumn{1}{l}{min}& \multicolumn{1}{r}{3}\\
			\multicolumn{1}{l}{max}& \multicolumn{1}{r}{11}\\
			\multicolumn{1}{l}{25\%}& \multicolumn{1}{r}{5}\\
			\multicolumn{1}{l}{50\%}& \multicolumn{1}{r}{6}\\
			\multicolumn{1}{l}{75\%}& \multicolumn{1}{r}{8}\\
			\hline\hline
		\end{tabular}
	\end{center}
	\label{table:genuine}
\end{table}

Like Tables \ref{table:bots} and \ref{table:genuine} show, there is a clear differences in the age attribute, between training and validation set.

We went forward to check if this diversity would had led us to a bad validation performance, or if the model would had handled the predictions in other ways.

The 10-fold crossvalidation on the validation set produced the following confusion matrix, with the correlated AUC score:\\

{
	\centering
	\begin{tabular}{@{}cc|cc@{}}
		\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
		\multicolumn{1}{c}{} & 
		\multicolumn{1}{c|}{} & 
		\multicolumn{1}{c}{BOT} & 
		\multicolumn{1}{c}{GEN}  \\
		\cline{2-4}
		\multirow[c]{2}{*}{Actual class}
		& BOT  & 558 & 2442\\
		& GEN  & 158 & 2842\\
		\cline{2-4}
		\multicolumn{2}{r|}{AUC} & 
		\multicolumn{2}{l}{0.566}\\
		\multicolumn{4}{c}{}\\
	\end{tabular}\\
}
The worsening were real, and it highlighted the short-sighted training phase we performed, trying to top the Botometer performance.

We tied to mitigate this performance loss, by excluding the main suspect from the features set.
Here is the validation performance, without considering the accounts' ages.

{
\centering
\begin{tabular}{@{}cc|cc@{}}
	\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted class} \\ 
	\multicolumn{1}{c}{} & 
	\multicolumn{1}{c|}{} & 
	\multicolumn{1}{c}{BOT} & 
	\multicolumn{1}{c}{GEN}  \\
	\cline{2-4}
	\multirow[c]{2}{*}{Actual class}
	& BOT  & 2920 & 20\\
	& GEN  & 1589 & 1411\\
	\cline{2-4}
	\multicolumn{2}{r|}{AUC} & 
	\multicolumn{2}{l}{0.721}\\
	\multicolumn{4}{c}{}\\
\end{tabular}\\
}

The improvement was encouraging, but still not enough to rely on this basic solution.
Considering the bot target as the positive class, we still had too many False Positive in our confusion Matrix. The binary classifier used to tend to identify an user as a bot, with too much confidence. We had to reduce that number, in order to provide a reliable filter in the final prediction pipeline system.

\subsection{Data extension}
The idea we had was to use some data from our multiclass dataset to enrich the binary training set, in order to make the algorithm handle younger and different types of samples from the Twitter population.

In order to perform the extension, we sampled 3,000 genuine accounts and 8,000 bots (2,000 content polluters for each class), all coming from our dataset, and added them to the Carvelee's dataset.
The new training set was composed by 42,212 samples.

We performed a 10-fold-crossvalidation to see the effect of this data refill, sticking to the same hyperparameters found by the last Grid Serch. The AUC score measured with these data was 0.963. We could see a slight improvement of the performances, with this data extension. However, we wanted to take a look inside the inner ranking performed by the algorithm, to check if the age field represented an important splitting point.
Figure \ref{fig:bon_importances_ext} shows that the age attribute was still the most considered when the trees had to perform the first splits.

We couldn't blindly follow the AUC score through Grid Searches, without make considerations about what will happen when we will allow people to classify data coming from outside our collected samples.
The age feature would had drove the Random Forest to misclassification over accounts with low \textit{age} values.
Even if the exclusion of that attribute would had made the overall AUC score worse, we had to strip it from the features vector, in order to better generalize on real test cases.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/bon_importances_extensions.png}
	\caption{Features ranking with augmented data - Top 12 }
	\label{fig:bon_importances_ext}
\end{figure}

While cross-validating the model, we tested the complete features vector (with and without the extension from our dataset) and the one stripped by the age values. The crossvalidation was performed with the same hyperparameters settings of the model fitted with the Carvelee's dataset only.

{
	\centering
	\begin{tabular}{@{}cccc@{}}
		\multicolumn{1}{c}{} & 
		\multicolumn{3}{c}{Fitted data} \\ 
		\cline{2-4}
		\multicolumn{1}{c|}{} & 
		\multicolumn{1}{c|}{original - with \textit{age} } & 
		\multicolumn{1}{c|}{extended - with \textit{age} } & 
		\multicolumn{1}{c|}{extended - without \textit{age}} \\
		\cline{1-4}
		\multicolumn{1}{|c|}{AUC} & 
		\multicolumn{1}{c|}{\textbf{0.961}} & 
		\multicolumn{1}{c|}{\textbf{0.963}} & 
		\multicolumn{1}{c|}{\textbf{0.948}} \\
		\cline{1-4}\\
	\end{tabular}\\
}

The age filed removing made thing worse, but we decided to perform it anyway, because of the good score reached without it, and the flexibility we were giving to the Random Forest. The slight worsening could be also imputed to the biased extrinsic features of that data: those samples came from the multiclass dataset and they originally had the extrinsic features based on the four bot categories' dictionaries.
In order to refill the binary dataset with these new samples, we had to recompute the extrinsic features, applying the analogous method used for the binary purpose.
We didn't recompute the entire dictionaries, we just assigned the scores to the new samples we were introducing, basing the calculations on the already listed words. Those had been exposed in chapter \ref{capitolo4}. This approach aimed to force the algorithm to identify bots and humans, basing its comparisons on the online computations of those features, like in a real-case generalization.

This last configuration was used to performed a further tuning of the parameters.
A new Grid Search brought us the configuration for the hyperparameters shown in Figure  \ref{fig:bon_tuning_refil}, leading to the new AUC score, exposed in Figure \ref{fig:bon_refil_auc}.
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/bon_tuning_refill.png}
	\caption{Grid Search with extended data}
	\label{fig:bon_tuning_refil}
\end{figure}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/refill_auc.png}
	\caption{AUC score with extended data}
	\label{fig:bon_refil_auc}
\end{figure}
The binary classifier has then been fitted with 42,212 samples with 34 features, 600 estimators, entropy splitting criterion and 26 levels of maximum depth.

\section{Multiclass ensemble classifier}
This is the first piece of the thesis we worked on.\\
It somehow represents the core of our thesis, it models the starting idea: go deep inside bot identification and search and classify similar behaviours among them.

In this section we will expose the model involved in the multiclass ensemble. In this process, we used a Random Forest algorithm, working on all the crafted features; a KNN model, operating on the user attributes only; a final text-based Naive Bayes classifier, which reads the tweets' texts and classifies them.

At first, this ensemble of those three models should had been blended with the prediction of the binary classifier. That means that the genuine class was part of the labels we were trying to classify, even in the multiclass models. Then, we found an issue in this approach: the binary classifier itself wasn't enough, even including it into the ensemble, to give the right importance to the genuine accounts. This problem emerged because of the others classifiers, as they were trying to classify the genuine class too. They lacked in data with that target, so, basically, they used to treat that category as one other of the bot types.

Even if the results on our validation sets were still good (we accomplished a F1 measure of 0.973), for the final ensemble method, we knew that this method would had yielded to a poor bot vs genuine detection tool. We couldn't accept that situation, because, in order to go deeper than other works, in bots' behaviours classifications, we had to provide a solid previous discrimination between humans and automated accounts.

The ensemble method with all the classifiers blended together were replaced with a pipeline, and the multiclass models were trained on bot categories only.
These last classifiers had been put together inside a stacking ensemble, which returns the final mutliclass probability prediction, based on the opinions of those models, as shown in Figure \ref{fig:stacking_schema}

The different nature of the classifiers, and the feature subsets as well, is one of the strengths of the stacking approach: it combines different opinions about the samples, driven by different classifiers, considering different parameters and attributes; basing on those unlike classifications, it builds its own.

It differs from other ensemble methods as bagging and boosting, because of this miscellaneous schema, and it can be a robust method to exploit the different characteristics of the classifiers stacked together.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/stacking.png}
	\caption{Multiclass ensemble schema}
	\label{fig:stacking_schema}
\end{figure}
In the following subsections there are the detailed explanations of the three classifier announced before.

\subsection{All-features-based Random Forest classifier}
\subsubsection{Dataset}
During this phase, we used the previously described dataset \ref{sec:dataset} with its five different labels.
The algorithm was fed with 21,445 samples and 37 features. the amount of records were light enough to consider K-fold crossvalidation, without slow the validation down too much.
\subsubsection{Model}
We found ourselves in the situation in which we had some brand new features and we didn't know how useful they were. Obviously, we could appeal to heath-maps or other tools, to highlight the correlations among variables and targets.
However, the model we wanted to develop was the Random Forest, which proved to perform well with F1 score. Since this kind of model exploits its criteria to employ the features, we needed to prove them with a direct approach.

\subsubsection{Features selection}
A useful advantage of the Random Forest algorithm is the ability to provide a feature ranking, according to its splitting criterion.
We retrieved this standing, in order to see if we would have found some of the ones coming out from feature engineering at the top positions.
The algorithm ranking ranked the features this way: 1. \textit{favourites\_count} (0.179115), 2. \textit{nsfw\_profile} (0.068165), 3. \textit{freq} (0.061246), 4. \textit{tweet\_intradistance} (0.060451), 5. \textit{news\_spreaders\_words\_score} (0.058583), 6. \textit{statuses\_count} (0.053364), 7. \textit{avg\_len} (0.051733), 8. \textit{followers\_count} (0.051187), 9. \textit{NSFW\_words\_score} (0.043496), 10. \textit{ret\_perc} (0.041200), 11. \textit{min\_len} (0.038870), 12. \textit{spam\_bots\_words\_score} (0.035445), ...  37. \textit{min\_fav} (0.000146).

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/top_12_features_importances.png}
	\caption{Random Forest top-12 feature ranking}
	\label{fig:feature_rank}
\end{figure}

As Figure \ref{fig:feature_rank} shows, we could find some of our crafted features inside this list: lots of tweets descriptive features (\textit{avg\_len, freq, ret\_perc}, etc...), as well as the \textit{tweet\_intradistance} attribute and three of the four extrinsic features, like \textit{news\_spreaders\_words\_score}, \textit{NSFW\_words\_score} and the \textit{spam\_bots\_words\_score}.
This picture confirmed us that the idea behind those features was useful.

Since those attributes were thought to belong to different clusters, we decided to try several combinations of those feature clusters, validating the model on them with a crossvalidation. The purpose of this stage was to see if some groups of features were enough to describe the real problem, or if some group would shown up as irrelevant.
To face this evaluation, we performed a light-weighted Grid Search, which is a method that takes desired ranges of hyperparameters and tries all the possible permutations of them, looking for the best combination, in terms of a certain metric.

We are talking about a light-weight version of this tool, because we just went through different numbers of tree estimators in the forest. The different feature groups are not considered as hyperparameters and are not handled by the Scikit-learn implementation of the Grid Search.
We had to manage the different training by our own, looking how the test score would have changed along with the increasing number of estimators and the different set of features.

Grid Search uses crossvalidation to find the better estimators for the models, and this approach was right for our situation.
Due to the multiclass nature and some imbalances with the labels, we decided to follow the F1 score metric to asses the value of our model.

The features were organized in clusters, as described in Chapter \ref{capitolo4}.
We had the user features, the descriptive features, the intrinsic features, the extrinsic and the image features. Then we tried the model with the entire set of 38 attributes.
As shown in Figure \ref{fig:feature_clusters}, the best configuration seems to involve the whole set of features, as it reaches these scores, with 100 estimators: \textit{Precision} = 0.978, \textit{Recall} = 0.976, \textbf{\textit{F1}}= 0.977.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/feature_cluster_f1.png}
	\caption{Performance over different feature clusters}
	\label{fig:feature_clusters}
\end{figure}

The model has been tested with the default value for the maximum depth in the trees, which is set to 'None'. It means that the trees are expanded until every leaf is pure, or all leaves contain one sample.

In order to try all the alternatives, we setted a test involving the performance of the model, when it was working on an increasing number of features.
We had the ranking provided by the forest itself, so we started by testing only the most important attribute, adding one feature at time, until the least important was included.
We were looking for some changing in the scores, that would had pointed to a lighter model, with the exclusion of some features.
Figures \ref{fig:feat_prec}, \ref{fig:feat_rec}, \ref{fig:feat_f1} show the trends of the Precision, the Recall and the F1, respectively, along with the number of features tested.
 \begin{figure}[htp!]
 	\centering
 	\includegraphics[width=\columnwidth]{chapter5/figure/precision_along_features.png}
 	\caption{Precision trend along with number of features tested}
 	\label{fig:feat_prec}
 \end{figure}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/recall_along_features.png}
	\caption{Recall trend along with number of features tested}
	\label{fig:feat_rec}
\end{figure}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/f1_along_features.png}
	\caption{F1 trend along with number of features tested}
	\label{fig:feat_f1}
\end{figure}

As all the Figures show, the best solution possible, looking at both the three metrics, is the one involving all the 37 components of the feature vector.
There was the possibility to choose the second result, which wanted only the first 28 features, in terms of importance for the Random Forest. However, we weren't struggling with heavy models or long prediction times and the Random Forest algorithm handles the overfitting problem properly, even with complex models.
Thus, we moved on with the entire feature vector as support for the classification goal.

We then continued with a proper Grid Search over the whole number of features.
\subsubsection{Hyperparameters Tuning}
The algorithms rely on parameters in order to fit a problem.
Once the number of featured was picked, as well as the model, we needed to consider the possible hyperparameter ranges. 
The Grid Search method from Scikit-learn helped us, one again, during this exploration.
Since we were testing a Random Forest, we wanted to play with the number of estimators (tree) to include in the pool, as well as the maximum depth of each tree and the splitting criterion.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/multiclass_rf_tuning_gini.png}
	\caption{F1 scores with 'Gini' criterion}
	\label{fig:rf_tuning_gini}
\end{figure}
\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/multiclass_rf_tuning.png}
	\caption{F1 scores with ''Entropy'' criterion}
	\label{fig:rf_tuning_entropy}
\end{figure}

The Figures \ref{fig:rf_tuning_gini}, \ref{fig:rf_tuning_entropy} show how the average F1 score, measured on 10-fold crossvalidation, changes with the increasing of the number of estimators in the forest.
The different coloured lines represent the \textit{max\_depth} hyperparameter.
The first Figure (\ref{fig:rf_tuning_gini}) shows the Grid Search results, with the \textit{gini} splitting criterion.
The second one (\ref{fig:rf_tuning_entropy}) represent the situation having \textit{entropy} as a splitting choice.
We combined nine numbers of estimators (200,250,300,350,400,450,500,550,600), together with three different maximum depths for the trees (26, 28, None) and the two above-mentioned splitting criteria.

We could observe a peak, for both criteria, in correspondence with 400 estimators.
Although the Gini-based forest's score didn't seem a bad point, we went with the Information Gain splitting criterion, which si also the same we used to rank the features of our data.

The final configuration involves 400 trees, the Information Gain criterion and the maximum reachable depth (for each of the 400 estimators) equal to 28 levels.

Figure (\ref{fig:tree}) shows an example of one of the estimators of the final model, plotted with Matplotlib library for Python. It has been represented with the first two levels of depth, for visualizations reasons.

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/tree.png}
	\caption{Tree estimator of the Random Forest model}
	\label{fig:tree}
\end{figure}

As the picture shows, this tree used the \textit{tweet\_intradistance} feature as root, in order to perform its first split on that attribute.

The first algorithm of the multiclass ensemble was completed and ready to be combined with the following models.

\subsection{User-based KNN classifier}
The Random Forest model represents somehow the core of the ensemble, as it was trained on the entire feature vector, with all the data we had for the purpose. A massive attention for parameters and features were given for that classifier. 
However, we wanted to put it into an ensemble, not to improve its already strong stability over outliers, but to support it with different perspectives.

We noticed, as shown in the previous Figure \ref{fig:feature_clusters}, that the user features were a good group to build a classifier on. Thus, we started thinking how to implement such support, and we basically looked at our baselines.

The model that had the best performance, not considering the Random Forest, was the K-Nearest Neighbors algorithm.

We didn't want this model to work the entire feature vector, because we knew that it would had been overlooked by the Random Forest. Instead, we wanted it to concentrate on the features that describe the users, without the information driven by their tweets. Therefore, we relied on the user features, with the extension of the image feature that assesses the NSFW score to the profile picture. This extension wa due to the fact that such feature doesn't need the user's tweets to be computed; it can be seen as one of the user features as the others of that group.

We hoped that treating the data before, or during, the training phase, would had brought to a good sustain for the first multiclass model, where needed.

\subsubsection{Dataset}
The dataset is composed by the same number (21,445) of samples of the first multitarget Random Forest, but preserving only the twelve features belonging to the user group, plus the NSFW\_profile attribute coming from the image features group.

\small
\begin{center}
	\begin{tabular}{ll}
		\\Feature vector\\
		\hline\hline
		default\_profile, favourites\_count, followers\_count\\
		friends\_count, listed\_count, screen\_name\_len\\
		statuses\_count, url, description\_len, NSFW\_profile\\
		name\_len, profile\_use\_background\_image, age\\\hline\\
	\end{tabular}
\end{center}
\normalsize

\subsubsection{Model}

This model is quite simple and doesn't require much effort in interpolating several hyperparameters. But this doesn't mean that it is a closed box algorithm. It can be improved by paying attentions to some details. In particular there had been done three considerations, and they were regarding

\begin{itemize}
	\item[\PencilRight]\textbf{Hyperparameters}\\
	The main hyperparameters that can be combined together are the number \textit{k} of neighbours ti consider, when performing a prediction, and the distance metric used by the calculations of the distances.
	The Scikit-learn implementation of the algorithm uses the \textit{Minkowski} metric, which describes the \textbf{L}\textsubscript{p} norm: 
	\[ \mathit{L_{p}} = (\sum\limits_{i = 1}^{n}|x_{i} - y_{i}|^{p})^{1/p} \]
	This metric is a generalization of the \textbf{L}\textsubscript{2} norm, also known as the Euclidean distance.
	The library we used allowed us to tune the \textit{p} hyperparameter, which indicates the metric used: \textit{p} = 1 means L\textsubscript{1} norm, the Manhattan Distance, when \textit{p} = 2 stands for the Euclidean.
	Higher values of \textit{p} are available, with other metric involved.
	
	We tried several values for both \textit{p} and \textit{k}, obtaining different results.
	However, the overall decision had to take in consideration also other two factors, the features weighting and the kernel function.
	
	\item[\PencilRight]\textbf{Features Weighting}\\
	The KNN algorithm aims to map the samples into a space and then it looks for the distances among them. In order to have best mapped space, it is reasonable to perform a weighting over the data. With this approach, the mapping would change and some points could result both as closer or more distant from each others.
	
	We performed some tests on four configurations. The first one didn't involve a weighting vector, while the second was a standard normalization of the features.
	This last option provided low results, with the same configurations of parameters, and thus were discarded from further analysis.

	The third option, which was the most interesting, was built by computing the Information gGin for each feature, using the inner ability of the Random Forest classifier, and then by applying the weights on the attributes, basing the coefficients on the scores provided by the ranking.
	This method emerged as the most effective, in the overall score.
	
	The last one was implemented as a Gaussian Kernel, which exploits the above-explained Radial Basis Function \ref{rbf}.
	This attempt fell shorter then the standard normalization, and it could be imputable to an overestimation of the ability to approximate the probability density of our data.
	It seemed that the Gaussian-based weighting wasn't a good fit for the problem.
\end{itemize}

We ran a Grid Search session over an increasing number of neighbours and the first four coefficients of the Minkowski distance.
All the combinations were tested with both the non-weighted data and the Entropy-based weighted data.
The results obtained are highlighted in Figure \ref{fig:knn_tuning}

\begin{figure}
	\includegraphics[width=\columnwidth]{chapter5/figure/knn_tuning.png}
	\caption{Hyperparameters p, k tuning, with different weightings}
	\label{fig:knn_tuning}
\end{figure}
The different colours represent the Minkowski distances tested, as well as the different line styles, dotted and continue, represent the weighted and the non-weighted solutions, respectively.

As visible in the picture, the best score, in the usual 10-fold crossvalidation, has been obtained with 5 neighbours, the Manhattan distance, and the weighted solution, which is averagely better than the other one.

The final model has been created with the following call: \textit{KNeighborsClassifier(p=1, n\_neighbors=5)} and it has been fitted with the weighted data.

\subsection{Text-based Naive Bayes classifier}
In chapter \ref{capitolo4} we created some useful features that helped the above-mentioned classifiers . Anyway we didn't consider enough the tweet texts. A first idea consisted in add a feature that describe the context of the tweets. This task was easily viable using \textit{Google Cloud Natural Language}. Unfortunately, this service only provides a few free calls, and we would not be able to tag all the tweets in our dataset. Moreover, the Context Classification is very specific, and we would have risked to have a too large domain of values. We tried to classify some tweets and we noticed that it was not possible with many of them, since they was just exclamation or contextless sentences. We therefore decided to train a proprietary text classifier. This allowed us to classify texts according to our targets, and our final classifier would have been self-sufficient, without external services.
\subsubsection{Dataset}
Since we wanted to classify texts instead of users, we needed to create a specific dataset.
It had to contain a list of tweets and all the related target. We composed it by labelling each tweet with the target of its author. This produced some noise; often bots tweet something different from their goal, to seem more humans. We could accept this problem, since most of the bot's tweets are aimed to a goal.
The shape of the dataset is the following:

\begin{center}
	\begin{tabular}{lr}
		category&\# tweets\\
		\hline\hline
		NSFW&196712\\
		news\_spreaders&280300\\
		spam&453719\\
		fake\_followers&41316\\
		genuine&261233\\
		\hline\\\\
	\end{tabular}
\end{center}

It seems unbalanced, anyway it is appropriate to keep all the data. Some spambot's tweets just contain links and they will be removed in the next steps. Differently, Fake-followers usually don't tweet, or they never tweeted.

\subsubsection{Model}
In order to classify texts, we decided to use a Naive Bayes approach. This algorithm consists in a probabilistic classifier based on the bayes' theorem. "Numerous researchers proved that it is effective enough to classify the text in many domains \cite{svm}. Naive Bayes models allow each attribute to contribute towards the final decision equally and independently from other attributes" \cite{nb}.

Tweets can't be processed as they are. Since this model aims to classify texts basing on its words, we had to clean the dataset from all those parts of the text that are not real or usefull words. For example articles, smiles, punctuaction should not be taken into consideration. Moreover it is fundamental to reduce inflected words to their word stem. Finally, we need to clean all those noisy parts of the tweets, which is important to allow the final text classifier to consider only real words, in order to identify the context.

Tweets without this pre-processing step look like this:\\

\framebox[\textwidth][l]{\parbox{\textwidth}{\textit{'RT @SteveSchmidtSES: TRUMP disgraced the Presidency and the\\United States at the G-7 summit. From his slovenly appearance to his\\unprepared... https://t.co/KiT29FvJw5'}}}\\

In order to perform this tasks, we used a \textit{sklearn Pipeline}. It is an object that contains intermediate steps of transformation and finally the machine learning algorithm. It works like a generic model, but it perform all the included transformations before processing a data, both in training and prediction steps.

We added to the pipeline the following operations:

\begin{itemize}
	\item[\PencilRight] \textbf{Remove retweet information:}\\
	Delete the texual pattern which indicates that the current status is a retweet. It consists in a "RT @original\_author:". 
	\item[\PencilRight] \textbf{Remove punctuation:}\\
	With regular expressions we removed everythink different from characters and numbers. In this step also smiles and other symbols are removed.
	\item[\PencilRight] \textbf{Remove stopwords:}\\
	stopwords are the most common words that are always used in a language and that can not help to classify a context. Some example of words belonging to this category are articles or prepositions.
	We removed them, by using a stopwords dictionary of \textit{NLTK} libraries.
	\item[\PencilRight] \textbf{Transform uppercase characters into lowercase:}\\
	Before tokenize words, we transformed every character into lowercase, in order to be sure that every word is considered only in one form.
	\item[\PencilRight] \textbf{Apply stemming:}\\
	This is the step where words are reduce to their word stem. Since the text classifier is based on the occurrences of words in the texts, we don't need a correct grammar in our tweets. Instead, words at their basic form are more useful for the target.
	In order to perform this transformation we used \textit{SnowballStemmer} from \textit{NLTK} libraries.
	\item[\PencilRight] \textbf{Apply TF-IDF encoding:}\\
	Finally we applied to each word a TF-IDF encoding. Since we had a huge amount of tweets, without this step we would have risked to give too much importance to the overused wordsand almost nothing importance to the others. Moreover, without using TF-IDF we had a worse performance.
	\item[\PencilRight] \textbf{MultinomialNB:}\\
	This is the final classification algorithm. Thanks to the pipeline it always receives cleaned data, performing a better training and predictions.
	We selected a Naive Bayes classifier for multinomial models since we are dealing with a multiclass problem.
\end{itemize}

\subsubsection{Holdout evaluation}
The final text classifier has the following performance:
\begin{itemize}
	\item[\PencilRight] \textbf{F1 = 0.71} with TF-IDF
	\item[\PencilRight] \textbf{F1 = 0.64} without TF-IDF
\end{itemize}

Anyway, since the other models classify users and not single tweets, we could not use a classifier for texts only.
In order to get a prediction on users, based only on their tweets, The final classification script compute the resulted probabilities for each tweet. Then, for each user, the final prediction consists in the mean of the predictions over his tweets.


\section{Stacking meta-classifier}
We had three models, each with different purposes, but they had to cooperate for the bots' behaviour identification.
The initial idea was to use only the multiclass Random Forest to classify the bot categories, using the other two models as meta-models to build extra features with their outcome.
Those features would have had the dataset enhanced, but their meaning would have been bounded to the multiclass classifier limits.
We wanted to give the right importance to each model, hoping they would help each other to better distinguish the patterns end to better model the real problem.

We thought about several methods to exploit their strengths and combine them. In particular, we thought about a genetic approach and a stacking ensemble with a meta-classifier. 
We wanted to evaluate the performance obtained by these methods and chose the one that fitted our need.

Both the genetic and the meta-model were trained with holdout technique, splitting the whole dataset into training and test sets. The 70\% of the samples ended up into the training set, the 30\% in the validation set.
We had a training set for the ensemble models that contains 6,434 entries.
The data that fed the stacking methods were the predictions of the tree classifiers, over the validation set.
In order to make those prediction without cheating, we couldn't use the models that were already fitted with the hole data. We had to retrain them with the 70\% of the records. 
We didn't perform further Grid Search to find the best hyperparameters in this stage, because the final script that we were going to assemble was taking into account the entire dataset to train the models. Furthermore, this small variation, in terms of amount of training data, wouldn't had led us into a misinterpretation of the problem, if we had kept the same hyperparameters found earlier.
We decided to stick with the configurations already found and to train the model with fewer data.

Once the model were fitted, we used the \textit{predict\_proba()} method of the Scikit-learn implementations of the classifiers, in order to retrieve "soft classifications".
We didn't want our model to assign a strict label to an unseen sample, indeed, we were interested in the percentage of categories membership.
The predict\_proba() method computes the probability, for a sample, to belong to the highlighted class, without assigning the most probable target.
We used this method to construct the output vectors needed to train the stacking models.

Each sample of this new dataset contains 12 elements, 4 soft predictions (one for each category) for each classifier (complete Random Forest, text-based Naive Bayes and user-based KNN).

\begin{center}
	\begin{tabular}{@{}c|c|c|c|c|c|c|c|c|c|c|c@{}}
		\multicolumn{12}{c}{New sample} \\
		\hline
		\multicolumn{4}{c|}{\textbf{KNN prob.}} & 
		\multicolumn{4}{c|}{\textbf{NB prob.}} & 
		\multicolumn{4}{c}{\textbf{RF prob.}}\\
		\hline
		\multicolumn{1}{c|}{p\textsubscript{0}} &
		\multicolumn{1}{c|}{p\textsubscript{1}} &
		\multicolumn{1}{c|}{p\textsubscript{2}} &
		\multicolumn{1}{c|}{p\textsubscript{3}} &
		\multicolumn{1}{c|}{p\textsubscript{4}} &
		\multicolumn{1}{c|}{p\textsubscript{5}} &
		\multicolumn{1}{c|}{p\textsubscript{6}} &
		\multicolumn{1}{c|}{p\textsubscript{7}} &	
		\multicolumn{1}{c|}{p\textsubscript{8}} &
		\multicolumn{1}{c|}{p\textsubscript{9}} &
		\multicolumn{1}{c|}{p\textsubscript{10}} &
		\multicolumn{1}{c}{p\textsubscript{11}}\\
		\hline
	\end{tabular}
\end{center}

A new training set was born and it was built with the soft classifications of the models in the pool, over the validation set. It was ready to proceed and to serve the ensemble models.\\

\begin{figure}[htp!]
	\centering
	\includegraphics[width=\columnwidth]{chapter5/figure/stacking_train.png}
	\caption{Training pipeline of the stacking ensemble}
	\label{fig:stacking_pipeline}
\end{figure}
The pipeline for perform the training of the ensemble models is resumed in Figure \ref{fig:stacking_pipeline}.

After several parallel attempts were done, the blending system was built with the meta-classifier.

\subsection{Genetic algorithm}
This approach started as a side way, when we were already testing the stacking ensemble.

The idea behind genetic programming, is to emulate the natural species evolution, by encoding the the \textit{chromosomes} in the process with data structures.
The chromosomes represent the possible solutions for the problem and they have to "evolve", in order to get fitter and fitter for the goal.
Several operators must be determined to perform this evolution.
Once a first \textit{generation} of feasible chromosomes has been formed, they have to be evaluate according to a \textit{fitness function}, which asses how well a chromosome faces the problem.
The best portion of chromosomes are picked to be part of the next generation, and this is called \textit{elitism}. The solutions left are given a probabilities to join the elite ones, in order to form a new generation with about the same size as the previous. This step is called \textit{selection}.
The chromosomes picked in the selection stage are assigned a high \textit{crossover} probability.
The crossover operator handles the "born" of new chromosomes, mixing parents alleles in a certain way. The mixing method is highly correlated to the chosen encoding strategy.
Each newborn is give a low probability to undergo a mutation. This step often seems useless, but it's pretty important, in order to explore a higher spectrum of solutions, which couldn't be expanded by the mating operators only.
After the new population has been accepted, it is ready to be validated through the previously define fitness function.
The loop holds, until a solution is found, or, like in our case, the process sticks to a local or global maximum.

\subsubsection{Genetic operators}

We setted the genetic algorithm with the support of the Deap library for Python, setting these operators:
\begin{itemize}
	\item[\PencilRight]\textit{Encoding}: each chromosomes represented a weighting vector for the outcome of our three classifiers. Each allele of the chromosome was float valued, with numbers between 0 and 5, generated randomly, with a uniform distribution. We started with normalized weights, but the spectrum of the solution explored was way too poor to fit the needs.
	This range was given after observing the weights that the Logistic Regression model were assigning to the inputs received, that was wider and involved even negative values.
	We randomly generated 200 chromosomes for the initial population, with this form.
\end{itemize}
\begin{center}
	\begin{tabular}{@{}c|c|c|c|c|c|c|c|c|c|c|c@{}}
		\multicolumn{12}{c}{Chromosome} \\
		\hline
		\multicolumn{4}{c|}{\textbf{KNN weights}} & 
		\multicolumn{4}{c|}{\textbf{NB weights}} & 
		\multicolumn{4}{c}{\textbf{RF weights}}\\
		\hline
		\multicolumn{1}{c|}{w\textsubscript{0}} &
		\multicolumn{1}{c|}{w\textsubscript{1}} &
		\multicolumn{1}{c|}{w\textsubscript{2}} &
		\multicolumn{1}{c|}{w\textsubscript{3}} &
		\multicolumn{1}{c|}{w\textsubscript{4}} &
		\multicolumn{1}{c|}{w\textsubscript{5}} &
		\multicolumn{1}{c|}{w\textsubscript{6}} &
		\multicolumn{1}{c|}{w\textsubscript{7}} &	
		\multicolumn{1}{c|}{w\textsubscript{8}} &
		\multicolumn{1}{c|}{w\textsubscript{9}} &
		\multicolumn{1}{c|}{w\textsubscript{10}} &
		\multicolumn{1}{c}{w\textsubscript{11}}\\
		\hline
	\end{tabular}
\end{center}
\begin{itemize}
	\item[\PencilRight]\textit{Fitness evaluation}: the fitness function that assessed the value of the solutions was somehow similar to the one used in the other stacking method.
	We applied the weights of our chromosomes to the samples in our dataset.
	
	For each sample, we made pairwise additions, among the outputs of different classifiers, multiplied by the chromosome's weights, for the same category:
	\begin{center}
		\begin{tabular}{@{}c@{}}
			
			\multicolumn{1}{c}{User-based components}\\
			\hline
			\multicolumn{1}{c}{p\textsubscript{0} * w\textsubscript{0} ... p\textsubscript{3} * w\textsubscript{3}}\\
			\hline\\
			\multicolumn{1}{c}{+}\\
			
			\\\multicolumn{1}{c}{Text-based components}\\
			\hline
			\multicolumn{1}{c}{p\textsubscript{0} * w\textsubscript{0} ... p\textsubscript{3} * w\textsubscript{3}}\\
			\hline\\
			\multicolumn{1}{c}{+}\\
			
			\\\multicolumn{1}{c}{All-features-based components}\\
			\hline
			\multicolumn{1}{c}{p\textsubscript{0} * w\textsubscript{0} ... p\textsubscript{3} * w\textsubscript{3}}\\
			\hline\\
			\multicolumn{1}{c}{=}\\
			
			\\\multicolumn{1}{c}{Resulting prediction}\\
			\hline
			\multicolumn{1}{c}{\textbf{p\textsubscript{0}} \textbf{p\textsubscript{1}} \textbf{p\textsubscript{2}} \textbf{p\textsubscript{3}}}\\
			\hline\\
		\end{tabular}
	\end{center}
	
	In order to stick to the probabilities nature, the computed prediction had been normalized.
	
	That prediction has been compared with the known real target for the examined sample.
	Since the targets of our dataset aren't soft valued, we took the maximum probability of the computed prediction to make the comparison with the actual class.
	Our fitness function aims to favourite those solutions which maximizes the F1 macro score, as it has been for the validation of the classifiers, until this stage.
	
	During this process, the problem we had to face was that we wanted to produce soft classifications, because we knew that our collected data presents similar patterns within the same categories. This means that the algorithms easily classify our test set, because of the distinctive traits found for each target. In order to mitigate the real test error, over unseen samples, we wanted the prediction to be as smooth as possible, without confusing the F1 score interpretation.
	
	We faced the problem involving a smoothing factor to our fitness function.\\
	When computing a sample, we populated a Confusion Matrix of the prediction, using the above-mentioned method to match predicted and actual classes. The matrix helped us computing the F1 macro score easily. At the same time, we counted every hard classification, marking as 'hard' every computed prediction that contained a probability greater or equal to \textbf{0.8}, among its five stored values.
	This count was used as a penalty, it has been averaged for the number of samples, and then subtracted to the computed F1 score of that chromosome. In order to privilege the maximization of the F1 factor, instead of the minimization of the penalty, the final fitness function assigned this score to each chromosome:
	\[ Fitness = 3 \times F1\_score - Penalty \]
	This way to operate didn't affect the overall F1 of the sample, since penalizing hard classifications didn't discourage the system to look for values high enough to have a dominant category in the prediction.
	
	Once every chromosome has been evaluated, they could proceed to the next steps of the algorithm.
	
	\item[\PencilRight]\textit{Selection}: the selection phase handles the choice over which chromosomes pick for mating. Several pre-implemented methods are available, but we used the tournament method. It works selecting the size \textit{K} of the tournament, which we chose to be 3. Then, it randomly selects \textit{K} (3) chromosomes from the population and places it inside a pool. Then it compares their fitness. The chromosome with the best fitness has probability \textit{p} (the crossover probability) to be selected for mating. The second has $ p*(1-p) $ chance to get selected, the third $ p*((1-p)^{2}) $.
	\item[\PencilRight]\textit{Crossover}: The crossover probability has been setted to 95\%.
	The crossover operator wasn't something already implemented by the library, as for the fitness function. Our operator used to produce two brand new chromosomes for the next generations.
	The first child is the unweighted mean of its parents:
	\[ [x_{0}, x_{1}, ... x_{11}] \oplus [y_{0}, y_{1}, ... y_{11}] = [\frac{x_{0}+y_{0}}{2}, \frac{x_{1}+y_{1}}{2}, ... \frac{x_{11}+y_{11}}{2}]\]
	The second child is the weighted mean of its parents, computing the weights with respect to the fitness of the two mating chromosomes:
	\[ f_{x} = \frac{fitness_{x}}{fitness_{x}+fitness_{y}} \]
	\[ f_{y} = \frac{fitness_{y}}{fitness_{x}+fitness_{y}} \]
	\[ [x_{0}, x_{1}, ..., x_{11}] \oplus [y_{0}, y_{1}, ..., y_{11}] = [x_{0}*f_{x}+y_{0}*f_{y}, ... x_{11}*f_{x}+y_{11}*f_{y} ]\]
	
	The retrieved children used to be part of the upcoming generation.
	\item[\PencilRight]\textit{Elitism}: This part was necessary, in order to not lose the best solutions found so far. It is a sort of insurance, which guarantees to keep, at least, the best situation until this stage, and to let it take part of the next generations of solution. We preserved our three best chromosomes for each generations and move them to the next stages.
	\item[\PencilRight]\textit{Mutation}: The mutation probability is generally setted to low values, like what happens in nature. It represent the error in DNA replications from the parents and it shouldn't reach the 1\% of probability to occur.
	Although, we wanted to force some mutation, because, as said before, we needed a wider space of solutions and the elitism helped us in containing the damages of such mutations. In the worst cases, all the chromosomes have had been damaged and resulted as useless, but the elitism had preserved the best ones and kept it untouched. So we imposed a 45\% of mutation probability, for each newborn solutions, before entering the pool.
	
	Our mutation operator was a decoration of the value changing method already implemented: we randomly used to pick three elements from the chromosome and set them to zero.
\end{itemize}

\subsubsection{Results}
After several runs of the genetic program, with boosted starts (the best solutions found at the previous run were placed inside the first generations of the following runs), we stuck in a maximum of the score.
In the last run, from the 5\textsubscript{th} generation there was no improvements in the fitness of the best solution. We selected the fittest chromosome, whose scores were:
\begin{itemize}
	\item[\PencilRight] \textit{Weights}:
	
	\begin{center}
		\begin{tabular}{@{}c|c|c|c@{}}
			\hline\hline
			\multicolumn{4}{c}{\textbf{KNN Weights}}\\
			\hline
			\multicolumn{1}{c|}{2.452}&
			\multicolumn{1}{c|}{4.104}&
			\multicolumn{1}{c|}{0.0}&
			\multicolumn{1}{c}{0.0}\\
			\hline
			\multicolumn{4}{c}{\textbf{NB Weights}}\\
			\hline
			\multicolumn{1}{c|}{4.766}&
			\multicolumn{1}{c|}{1.0}&
			\multicolumn{1}{c|}{0.0}&
			\multicolumn{1}{c}{0.0}\\
			\hline
			\multicolumn{4}{c}{\textbf{RF Weights}}\\
			\hline
			\multicolumn{1}{c|}{3.790}&
			\multicolumn{1}{c|}{1.0}&
			\multicolumn{1}{c|}{1.08}&
			\multicolumn{1}{c}{2.506}\\
			\hline\hline
		\end{tabular}
	\end{center}
	\item[\PencilRight] \textit{Fitness}: 0.768
	\item[\PencilRight] \textit{F1 score}: 0.44
	\item[\PencilRight] \textit{Percentage of hard classifications}: 55.3\%
\end{itemize}

The results were discouraging, compared to the singular scores of the models involved.
It was worth to try this approach, but we were aware that a "simple" weighted mean of the outcomes of the classifiers weren't enough to describe the problem.

Thus, we built a different and more sophisticated stacking method.

\subsection{Logistic Regression}
The reason behind the choice of a meta-classifier is that we wanted a more complex way to perform inner weighting of the outcomes that we had from other models.
A simple weighted mean wasn't enough for this purpose. Furthermore, implementing a logistic-like loss to evaluate the fitness of a genetic algorithm would have mean to apply the Logistic Regression training model, without performing gradient descent, but with a genetic approach. It would have been just unnecessary and computationally expensive.
Thus, we discarded the idea of using the genetic programming to emulate a Logistic model, even if the smoothing factor used for that try was a good insight for our task.
In order to mitigate the lacking of soft classifications, we chose to rely on the regularization factors that belong to the training algorithm of the Logistic Regression.
This kind of models is often involved in stacking other classifiers, with a binary purpose.

\subsubsection{Dataset}
The same training set has been used for train both the Genetic and Logistic models.
Since we were managing a multilabel datasets, we knew that the ensemble meta-model would had been adapted to this job. The most common tool used for stacking purposes is the Logistic Regression, which performs well on binary separations. We decided to test this model on a multinomial approach, with a softmax activation function, instead of trying the already visited One-vs-Rest method.

\subsubsection{Comparison with Random Forest}
We didn't wanted to blindly select this model over some others tool, especially over Random Forest, which proved us to perform well in multiclass classifications.
Thus, we tested these two algorithms with the new dataset.
We ran some default configurations of the models, in order to have a raw comparison to trace a line between them.

\begin{figure}[htp!]
	\begin{multicols}{2}
		\includegraphics[width=\linewidth]{chapter5/figure/logreg_default.png}\par 
		\caption{LogReg with raw settings}
		\label{fig:lr_def}
		\includegraphics[width=\linewidth]{chapter5/figure/random_forest_default.png}\par 
		\caption{Random Forest with raw settings}
		\label{fig:rf_def}
	\end{multicols}
	\caption{Stacking models comparison}
\end{figure}

Figure \ref{fig:lr_def} shows the early convergence of the Logistic model's F1 score, with low maximum iterations. The model has been tested with Lasso penalty and two different solvers, but the results, over the increasing of the training epochs, are way similar.
Although the Random Forest, as shown in Figure \ref{fig:rf_def}, tops the performance of the Logistic Regression, the scores were close enough (F1\textsubscript{LR} = 0.9868,  F1\textsubscript{RF} = 0.9874) to give a chance to the Logistic model, in order to try its regularization terms.

\subsubsection{Hyperparameters tuning}
We tried two regularization terms for the Logistic model and several numbers of maximum iterations for the training algorithms.
The regularization terms are parameters computed in addition with the minimization of the characteristic loss function. Their purpose is to avoid the weights to explode and the model to become more sensitive to noisy data. In other words, they are involved to prevent overfitting.
The idea is that the loss function, gets modified as follows
\[ \mathbf{L(w)} = L_{D}(w) + \lambda L_{W}(w)\]
Where $ L_{D}(w)  $ represent the error on the data and $ L_{W}(w)  $ is the term representing the model complexity. In general, smoother weights implies lower model complexity. The lighter the complexity, the lower the variance of the model and the risk perform overfitting.
The parameter $ \mathit{\lambda} $ has to be tuned with a validation method.

The penalties that we explored were:
\begin{itemize}
	\item[\PencilRight] \textit{Lasso (L\textsubscript{1})}:
	\[ \mathbf{L_{1}(w) }= \frac{\lambda}{2} ||\mathbf{w}||_{1} \]
	where $ ||\mathbf{w}||_{1} = \sum_{i=1}^{N}|w_{i}| $
	
	This regularization function is non-linear and doesn't provide a closed-form solution. It tends to cut out some features from the model, yielding to sparse and lighter model. It can be seen as an implicit way to apply features selection.
	
	\item[\PencilRight] \textit{Ridge (L\textsubscript{2})}:
	\[ \mathbf{L_{2}(w) }= \frac{\lambda}{2} ||\mathbf{w}||^{2}_{2} \]
	where $ ||\mathbf{w}||^{2}_{2} = \sum_{i=1}^{N}w_{i}^{2} $
	
	This softer term tends to shrink the weights, keeping the loss function quadratic in \textbf{w} and closed forum solution exists.
\end{itemize}
\begin{figure}
	\includegraphics[width=\columnwidth]{chapter5/figure/logreg_l1_lambda.png}\par 
	\caption{Lasso, $ \lambda$ = [0.1, 1, 10, 100]}
	\label{fig:lr_lasso_lambda}
\end{figure}
\begin{figure}
	\includegraphics[width=\columnwidth]{chapter5/figure/logreg_l2_lambda.png}\par 
	\caption{Ridge, $ \lambda$ = [0.1, 1, 10, 100}
	\label{fig:lr_ridge_lambda}
\end{figure}

Figure \ref{fig:lr_lasso_lambda} highlights the slightly better results obtained with the Lasso penalty, with unitary $ \lambda $ coefficient (Lasso F1 score: 0.973).
As Figure \ref{fig:lr_ridge_lambda} shows, the ridge penalty needs to be weakened ($ \lambda $ = 0.1) in order to get close to the Lasso performance, which is a compromise hard to deal with. The smaller  is the regularization coefficient, the higher is the model complexity, as said before.
Moreover, we decided to gather further consideration, by looking inside the weighting applied by those two terms.
Since we didn't have lot of data for the training, we wanted to keep the regularization high enough to not fit the noise in the model.

We took a look inside the weighting performed by the model, with both L\textsubscript{1} and L\textsubscript{2} regularizations, in order to catch some insight from them.
The weights are composed by four vectors of twelve elements each: each vector represent the weights applied for a One-vs-Rest target classification, and each element of the vectors are mark the features the model has been fitted on. As said before, each of the three groups of four features represents the probabilities, for a sample, of membership to the classes.
In order to give a good representation, we considered just one vector of twelve elements, computed by averaging all the weights applied in all the OvR predictions.

\begin{center}
	\begin{tabular}{@{}cccc@{}}
		\multicolumn{4}{c}{Ridge regularization}\\
		\hline\hline
		\multicolumn{4}{c}{\textbf{KNN mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ 3.4e^{-16} $}&
		\multicolumn{1}{c|}{$ 1.7e^{-15} $}&
		\multicolumn{1}{c|}{$ 3.2e^{-15} $}&
		\multicolumn{1}{c}{$ -1.1e^{-14} $}\\
		\hline
		\multicolumn{4}{c}{\textbf{Naive Bayes mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ 2.8e^{-15} $}&
		\multicolumn{1}{c|}{$ -1.1e^{-14} $}&
		\multicolumn{1}{c|}{$ 9.8e^{-15} $}&
		\multicolumn{1}{c}{$ 4.8e^{-15} $}\\
		\hline
		\multicolumn{4}{c}{\textbf{Random Forest mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ 8.7e^{-15} $}&
		\multicolumn{1}{c|}{$ -8.3e^{-15} $}&
		\multicolumn{1}{c|}{$ 1.0e^{-14} $}&
		\multicolumn{1}{c}{$ -1.7e^{-14} $}\\
		\hline\hline\\
	\end{tabular}
\end{center}
The Ridge regularization leads to very small weights, and negative ones too. Even with unitary $ \lambda  $ coefficient, it is hard to distinguish a discrimination among features. This approach would have yielded a smoother model, but with the ability to give a chance to every classifier to distinguish among targets.
We wanted to get some further insights from the other weighting.

\begin{center}
	\begin{tabular}{@{}cccc@{}}
		\multicolumn{4}{c}{Lasso regularization}\\
		\hline\hline
		\multicolumn{4}{c}{\textbf{KNN mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ -0.660 $}&
		\multicolumn{1}{c|}{$ -0.185 $}&
		\multicolumn{1}{c|}{$ -0.465 $}&
		\multicolumn{1}{c}{$ -0.830 $}\\
		\hline
		\multicolumn{4}{c}{\textbf{Naive Bayes mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ 0.578 $}&
		\multicolumn{1}{c|}{$ -0.054 $}&
		\multicolumn{1}{c|}{$ 0.072 $}&
		\multicolumn{1}{c}{$ 0.0 $}\\
		\hline
		\multicolumn{4}{c}{\textbf{Random Forest mean weights}}\\
		\hline
		\multicolumn{1}{c|}{\textit{NSFW}}&
		\multicolumn{1}{c|}{\textit{NS}}&
		\multicolumn{1}{c|}{\textit{SB}}&
		\multicolumn{1}{c}{\textit{FF}}\\
		\hline
		\multicolumn{1}{c|}{$ 1.993 $}&
		\multicolumn{1}{c|}{$ 2.147 $}&
		\multicolumn{1}{c|}{$ 2.049 $}&
		\multicolumn{1}{c}{$ 2.571 $}\\
		\hline\hline\\
	\end{tabular}
\end{center}
The L\textsubscript{1} term, as expected cut out some features from the model.
Looking at the excluded attributes, we noticed that the regularization caught the strenghts and the weaknesses of the classifiers. The text-based Naive Bayes classifier seemed to be useless when it came to detect Fake-Followers. It seemed legit, as the dictionary used by that category is the smallest and the most heterogeneus (lots of non english words are involved) in our dataset.
Lasso seemed to understand this behaviour and decided to not consider the opinion of that classifier, when it has to give its opinion about that bot cateogry.
Another insight got from L\textsubscript{1} is coming again from the Naive Bayes algorithm. The model couldn't distinguish with certainty Spam-Bots, since they act in a similar way, with respect to NSFW accounts. They just tweets click-baiting links, with catchy captions. A "blind" classifier struggles in understand the nature of those links. This is the reason that the contributes of the tex-based model had been almost discarded from the stacking meta-classifier.

\begin{figure}
	\begin{multicols}{2}
		\includegraphics[width=\linewidth]{chapter5/figure/logreg_l1_close.png}\par 
		\caption{Up to 150 iterations}
		\label{fig:lr_lasso_close}
		\includegraphics[width=\linewidth]{chapter5/figure/logreg_l1_far.png}\par 
		\caption{Up to 5000 iterations}
		\label{fig:lr_lasso_far}
	\end{multicols}
	\caption{Lasso Logistic Regression scores over solvers}
\end{figure}

Since we knew our dataset and we were aware of the bias it might contain, we preferred a lighter and sparser model, over a more complex one, even when the F1 scores used to match. We wanted our model to infer on new unseen data and to be ready to give a representative statistical description of the actual situation on Twitter.
We had to be far-sighted and not to recline on the accomplishments of the 10-fold crossvalidation. We thought that the Lasso model would had been performing better in out-of-box predictions.

We kept the L\textsubscript{1} penalty, with $ \lambda  = 1 $   and proceeded with the hyperparameters tuning.

Figure \ref{fig:lr_lasso_close} shows the trend in the F1 score, along with the increasing number of iterations, applying the \textit{SAGA}\cite{SAGA} solver (a variant of the \textit{Stochastic Average Gradient}\cite{SAG} optimization that supports Lasso penalty) and the \textit{LIBLINEAR}\cite{Liblinear}, an open source library for large-scale linear classification.

As it can be seen in Figures \ref{fig:lr_lasso_far}, by increasing the number of maximum iterations, up to 5000, the performances remain stable with every solver.
The algorithms seem to not improve after 75 maximum iterations setted.
Moreover, the LIBLINEAR solver gains slightly better results, in terms of F1 score, as it reaches 0.9867 in this metric, compared with the score obtained with LIBLINEAR solver, which is 0.9867.

The final Logistic Regression meta-classifier has been fitted with the Scikit-learn library, with this setting:\\
\textit{LogisticRegression(max\_iter = 100, penalty = "l1", solver = "liblinear", C = 1, multi\_class = "multinomial", fit\_intercept = True)}.\\
The \textit{C} parameter stands for $ \frac{1}{\lambda} $, the regularization coefficient.
This setting obtained the following scores in a 10-fold crossvalidation:
\begin{itemize}
	\item[\PencilRight] \textit{Precision}: 0.987
	\item[\PencilRight] \textit{Recall}: 0.986
	\item[\PencilRight] \textit{F1 score}: 0.986
\end{itemize}
\section{Prediction pipeline}
\label{predicion_pipeline}
The final model is represented by an execution pipeline, involving a first binary classifier and then a mutliclass ensemble.

As described by figure \ref{fig:prediction_pipeline}, In order to perform a prediction over a new sample the process is the following:
\begin{itemize}
	\item[\PencilRight]User and tweets data retrieving with Twitter APIs
	\item[\PencilRight]Prepare data with binary extrinsic features and strip image attributes, in order to output binary probability prediction (\textbf{Binary Random Forest})
	\item[\PencilRight]Prepare data and perform features engineering to output multiclass probability prediction (\textbf{All-features multiclass Random Forest})
	\item[\PencilRight]Strip all attributes, except for the user features, weight them with Information Gain-driven proportions to output multiclass probability prediction (\textbf{User-based multiclass KNN})
	\item[\PencilRight]Prepare and treat text to perform text-based probability prediction (\textbf{Text-based multiclass Naive Bayes})
	\item[\PencilRight]Build new features vector with the stacked outcomes of the mutliclass classifiers
	\item[\PencilRight]Compute the final mutliclass probabilistic prediction with the meta-classifier (\textbf{Multinomial Logistic Regression})
	\item[\PencilRight]Use the multiclass division to repartition the bot probability provided by the binary model
\end{itemize}
The binary classifier returns two values: the membership probability for the bot category and the one for the genuine class.
The percentage that marks the bot nature of the examined account gets partitioned by the outcome of the mutliclass stacking ensemble.
The pipeline will be performed by a web application, in order to provide a useful classification tool for every internet user.
The engine of this web application is mainly composed by a python script, which, given a Twitter user name, resumes this prediction pipeline and executes it, providing the classification.

This last lines anticipate the content of the following chapter of our thesis.
\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\columnwidth]{chapter5/figure/pred_pipeline.png}\par 
	\end{center}
	\caption{Final prediction pipeline}
	\label{fig:prediction_pipeline}
\end{figure}