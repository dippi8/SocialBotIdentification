{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import tweepy\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "user = 'politicalfacts'\n",
    "\n",
    "\n",
    "# create connection with Twitter API\n",
    "\n",
    "CONSUMER_KEY = 'J3uhwWHdmSt3uD69ry8r2kc3B'\n",
    "CONSUMER_SECRET = 'LeuM7dVfFbXH0bc5fFkZIxlNiUXyjIx4Kcjh58HeWs2TrLijDx'\n",
    "ACCESS_TOKEN = '327497511-5dNjiGaTQHASljoxNr1qlMOlHrrSB21HBFBDjx2E'\n",
    "ACCESS_TOKEN_SECRET = 'tHLqwlhHYJWptW3femyq0rbMW6ZItu5yLQ3DFGJqJ8Xlg'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "\n",
    "# download user's tweets\n",
    "\n",
    "stuff = api.user_timeline(screen_name = user, count = 100, include_rts = True, tweet_mode=\"extended\")\n",
    "tweets = []\n",
    "\n",
    "for tweet in stuff:\n",
    "    tweet._json['user_id'] = tweet._json['user']['id']\n",
    "    if len(tweet._json['entities']['urls']) != 0:\n",
    "        tweet._json['url'] = tweet._json['entities']['urls'][0]['expanded_url']\n",
    "    else:\n",
    "         tweet._json['url'] = None\n",
    "    del tweet._json['user'], tweet._json['entities']\n",
    "\n",
    "    tweets.append(tweet._json)\n",
    "        \n",
    "tweets = pd.DataFrame.from_dict(tweets)\n",
    "\n",
    "\n",
    "# download user's features\n",
    "\n",
    "user = api.get_user(user)\n",
    "\n",
    "user_features = pd.DataFrame([[user.id,\n",
    "                    user.name,\n",
    "                    user.screen_name,\n",
    "                    user.statuses_count,\n",
    "                    user.followers_count,\n",
    "                    user.friends_count,\n",
    "                    user.favourites_count,\n",
    "                    user.listed_count,\n",
    "                    user.url,\n",
    "                    user.lang,\n",
    "                    user.time_zone,\n",
    "                    user.location,\n",
    "                    user.default_profile,\n",
    "                    user.default_profile_image,\n",
    "                    user.geo_enabled,\n",
    "                    user.profile_image_url,\n",
    "                    user.profile_use_background_image,\n",
    "                    user.profile_background_image_url_https,\n",
    "                    user.profile_text_color,\n",
    "                    user.profile_image_url_https,\n",
    "                    user.profile_sidebar_border_color,\n",
    "                    user.profile_background_tile,\n",
    "                    user.profile_sidebar_fill_color,\n",
    "                    user.profile_background_image_url,\n",
    "                    user.profile_background_color,\n",
    "                    user.profile_link_color,\n",
    "                    user.utc_offset,\n",
    "                    user.is_translator,\n",
    "                    user.follow_request_sent,\n",
    "                    user.protected,\n",
    "                    user.verified,\n",
    "                    user.notifications,\n",
    "                    user.description,\n",
    "                    user.contributors_enabled,\n",
    "                    user.following,\n",
    "                    user.created_at]],\n",
    "                    columns=[\"id\",\"name\",\"screen_name\",\"statuses_count\",\"followers_count\",\"friends_count\",\"favourites_count\",\"listed_count\",\"url\",\"lang\",\"time_zone\",\"location\",\"default_profile\",\"default_profile_image\",\"geo_enabled\",\"profile_image_url\",\"profile_use_background_image\",\"profile_background_image_url_https\",\"profile_text_color\",\"profile_image_url_https\",\"profile_sidebar_border_color\",\"profile_background_tile\",\"profile_sidebar_fill_color\",\"profile_background_image_url\",\"profile_background_color\",\"profile_link_color\",\"utc_offset\",\"is_translator\",\"follow_request_sent\",\"protected\",\"verified\",\"notifications\",\"description\",\"contributors_enabled\",\"following\",\"created_at\"]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context score\n",
    "\n",
    "porn_words = pd.read_csv('../scripts/data/porn/filtered_main_words.csv', sep=',')\n",
    "prop_words = pd.read_csv('../scripts/data/propaganda/filtered_main_words.csv', sep=',')\n",
    "spam_words = pd.read_csv('../scripts/data/spam/filtered_main_words.csv', sep=',')\n",
    "fake_words = pd.read_csv('../scripts/data/fake_followers/filtered_main_words.csv', sep=',')\n",
    "genuine_words = pd.read_csv('../scripts/data/genuine/filtered_main_words.csv', sep=',')\n",
    "\n",
    "def compute_score(tweets):\n",
    "\n",
    "    user_score = pd.DataFrame(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score'])\n",
    "\n",
    "    for tweet in tweets['full_text']:\n",
    "        # check for words in main_words and compute the scores for each tweet and for each category\n",
    "        mask = np.in1d(porn_words.word, tweet.split())\n",
    "        porn_score = porn_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(prop_words.word, tweet.split())\n",
    "        prop_score = prop_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(spam_words.word, tweet.split())\n",
    "        spam_score = spam_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(fake_words.word, tweet.split())\n",
    "        fake_score = fake_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(genuine_words.word, tweet.split())\n",
    "        genuine_score = genuine_words.loc[mask]['score'].values.sum()\n",
    "        \n",
    "        user_score = user_score.append(pd.DataFrame({'porn_words_score': porn_score, 'prop_words_score': prop_score, 'spam_words_score': spam_score,'fake_words_score': fake_score,'genuine_words_score': genuine_score}, index=[0]),sort=False, ignore_index=True)\n",
    "\n",
    "    return user_score\n",
    "\n",
    "\n",
    "if len(tweets) > 0:\n",
    "    # sum all the scores of each category\n",
    "    user_score = compute_score(tweets).sum()\n",
    "    scores = np.divide(user_score,len(tweets))\n",
    "else:\n",
    "    scores = pd.DataFrame({'porn_words_score': 0, 'prop_words_score': 0, 'spam_words_score': 0, 'fake_words_score': 0, 'genuine_words_score': 0}, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute intradistances\n",
    "\n",
    "def compute_centroid(tf_idf):\n",
    "\n",
    "    center = tf_idf.sum(axis=1)/tf_idf.shape[0]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_centroid(tf_idf, centroid):\n",
    "    \n",
    "    distances = []\n",
    "    for elem in tf_idf:\n",
    "        distances.append(np.linalg.norm(tf_idf - centroid))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wss(id, tweets_df, is_tweet = 1):\n",
    "    \n",
    "    if is_tweet == 1:\n",
    "        # get tweets per id\n",
    "        vector = tweets_df['full_text']\n",
    "        n_vectors = len(vector)\n",
    "    elif is_tweet == 0:\n",
    "        # get domains per id\n",
    "        vector = tweets_df['url']\n",
    "        vector = vector.fillna('').astype(str)\n",
    "        for i in range(len(vector)):\n",
    "            vector.iloc[i] = urlparse(vector.iloc[i]).netloc\n",
    "        n_vectors = len(vector)\n",
    "    else:\n",
    "        print ('Invalid Input')\n",
    "\n",
    "    transformer = TfidfVectorizer(smooth_idf=True)\n",
    "    tf_idf = transformer.fit_transform(vector).todense()\n",
    "    \n",
    "    centroid = compute_centroid(tf_idf)\n",
    "    distances = dist_from_centroid(tf_idf, centroid)\n",
    "    avg_dist = np.asarray(distances).sum()/n_vectors\n",
    "    \n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intradistances():\n",
    "    try:\n",
    "        tw = (wss(user, tweets, 1))\n",
    "    except:\n",
    "        tw = 0\n",
    "        \n",
    "    try:\n",
    "        url = (wss(user, tweets, 0))\n",
    "    except:\n",
    "        url = 0\n",
    "        \n",
    "    return  pd.DataFrame({'tweets_intra': tw, 'url_intra': 0}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistances = intradistances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect descriptive features\n",
    "\n",
    "def describe_tweets(tweets):\n",
    "    \n",
    "    ret_perc, media_perc, url_perc, quote_perc = tweet_perc(tweets)\n",
    "    \n",
    "    avg_len, avg_ret, avg_fav, avg_hash = tweet_desc(tweets, 'avg')\n",
    "    max_len, max_ret, max_fav, max_hash = tweet_desc(tweets, 'max')\n",
    "    min_len, min_ret, min_fav, min_hash = tweet_desc(tweets, 'min')\n",
    "    \n",
    "    freq = tweet_freq(tweets)\n",
    "\n",
    "    frame = np.array([avg_len, max_len, min_len, avg_ret, max_ret, min_ret, avg_fav, max_fav, min_fav, avg_hash, max_hash, min_hash, freq, ret_perc, media_perc, url_perc, quote_perc])\n",
    "   \n",
    "    desc_features = pd.DataFrame({'avg_len': avg_len, 'max_len': max_len, 'min_len': min_len, 'avg_ret': avg_ret, 'max_ret': max_ret, 'min_ret': min_ret, 'avg_fav': avg_fav, 'max_fav': max_fav, 'min_fav': min_fav, 'avg_hash' : avg_hash, 'max_hash' : max_hash, 'min_hash' : max_hash,'freq': freq, 'ret_perc': ret_perc, 'media_perc': media_perc, 'url_perc': url_perc, 'quote_perc': quote_perc}, index=[0])\n",
    "    \n",
    "    return desc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_perc(tweets):\n",
    "    \n",
    "    try:\n",
    "        ret_perc = np.invert(tweets.retweeted_status.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        ret_perc = 0\n",
    "    try:\n",
    "        media_perc = np.invert(tweets.extended_entities.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        media_perc = 0\n",
    "    try:\n",
    "        url_perc = np.invert(tweets.url.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        url_perc = 0\n",
    "    try:\n",
    "        quote_perc = tweets.is_quote_status.sum()/len(tweets)\n",
    "    except:\n",
    "        quote_perc = 0\n",
    "    \n",
    "    return ret_perc, media_perc, url_perc, quote_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_count(tweets):\n",
    "    \n",
    "    occurrences = []\n",
    "    for tweet in tweets:\n",
    "        occurrences.append(tweet.count('#'))\n",
    "        \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_desc(tweets, metric):\n",
    "    \n",
    "    tweets_lenght = tweets['full_text'].apply(lambda x: len(x))\n",
    "    \n",
    "    if metric == 'avg':\n",
    "        ret = np.mean(tweets.retweet_count)\n",
    "        lenght = np.mean(tweets_lenght)\n",
    "        fav = np.mean(tweets.favorite_count)\n",
    "        hashtag = np.mean(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'max':\n",
    "        ret = max(tweets.retweet_count)\n",
    "        lenght = max(tweets_lenght)\n",
    "        fav = max(tweets.favorite_count)\n",
    "        hashtag = max(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'min':\n",
    "        ret = min(tweets.retweet_count)\n",
    "        lenght = min(tweets_lenght)\n",
    "        fav = min(tweets.favorite_count)\n",
    "        hashtag = min(hashtag_count(tweets['full_text']))\n",
    "\n",
    "    return lenght, ret, fav, hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_freq(tweets):\n",
    "    \n",
    "    dates = list(tweets.created_at)\n",
    "    \n",
    "    last = dates[0]\n",
    "    d = last[8:10]\n",
    "    m = last[4:7]\n",
    "    y = last[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    last = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    first = dates[-1]\n",
    "    d = first[8:10]\n",
    "    m = first[4:7]\n",
    "    y = first[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    first = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    delta = (last - first).days + 1\n",
    "    freq = len(tweets)/delta\n",
    "    \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(tweets_df):\n",
    "    \n",
    "    tweets = tweets_df\n",
    "    \n",
    "    if len(tweets) > 0:\n",
    "        # sum all the scores of each category\n",
    "        features = describe_tweets(tweets)\n",
    "    else:\n",
    "        features = pd.DataFrame({'avg_len': 0, 'max_len': 0, 'min_len': 0,'avg_ret': 0, 'max_ret': 0, 'min_ret': 0, 'avg_fav': 0, 'max_fav': 0, 'min_fav': 0, 'avg_hash': 0, 'max_hash': 0, 'min_hash': 0, 'freq': 0, 'ret_perc': 0, 'media_perc': 0, 'url_perc': 0, 'quote_perc':0}, index=[0])\n",
    "    \n",
    "    # return the average scores of each user\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = describe(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([user_features, intradistances, scores, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF classification\n",
    "\n",
    "def oldness(x):\n",
    "    x = str(x)\n",
    "    if x[0] == '2':\n",
    "        return 2018 - int(x[:4])\n",
    "    else:\n",
    "        return 2018 - int(x[-4:])\n",
    "    \n",
    "full = full.drop(columns=['contributors_enabled', 'follow_request_sent', 'following', 'profile_background_image_url', 'profile_background_image_url_https', 'profile_image_url', 'profile_image_url_https', 'time_zone', 'utc_offset'])\n",
    "full = full.drop(columns=['default_profile_image','is_translator', 'geo_enabled', 'location', 'notifications', 'profile_background_tile', 'protected', 'verified'])\n",
    "full['default_profile'] = full['default_profile'].fillna(full['default_profile'].mode()[0])\n",
    "full['description'] = full['description'].fillna('')\n",
    "full['description_len'] = full['description'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['description'])\n",
    "full['name'] = full['name'].fillna('')\n",
    "full['name_len'] = full['name'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['name'])\n",
    "full['screen_name'] = full['screen_name'].fillna('')\n",
    "full['screen_name_len'] = full['screen_name'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['screen_name'])\n",
    "full = full.drop(columns=['lang'])\n",
    "full['age'] = full['created_at'].apply(lambda x: oldness(x))\n",
    "full = full.drop(columns=['created_at'])\n",
    "full = full.drop(columns=['id'])\n",
    "full['profile_use_background_image'] = full['profile_use_background_image'].fillna(full['profile_use_background_image'].mode()[0])\n",
    "full['url'] = (full['url'].notnull()).astype(int)\n",
    "full = full[['avg_fav', 'avg_hash', 'avg_len', 'avg_ret', 'default_profile',\n",
    "       'fake_words_score', 'favourites_count', 'followers_count', 'freq',\n",
    "       'friends_count', 'genuine_words_score', 'listed_count', 'max_fav',\n",
    "       'max_hash', 'max_len', 'max_ret', 'media_perc', 'min_fav', 'min_hash',\n",
    "       'min_len', 'min_ret', 'porn_words_score',\n",
    "       'profile_use_background_image', 'prop_words_score', 'quote_perc',\n",
    "       'ret_perc', 'spam_words_score', 'statuses_count',\n",
    "       'tweets_intra', 'url', 'url_intra', 'url_perc',\n",
    "       'description_len', 'name_len', 'screen_name_len', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pickle.load(open('../scripts/rf.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores = rf.predict_proba(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoN classification\n",
    "\n",
    "full.drop(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score', 'genuine_words_score'], inplace=True)\n",
    "\n",
    "# predict bot or not\n",
    "\n",
    "bon = pickle.load(open('../scripts/bot_or_not.model', 'rb'))\n",
    "bon_scores = bon.predict_proba(full)\n",
    "\n",
    "bon_scores = pd.DataFrame(bon_scores, columns=['bon_4', 'bon_3'])\n",
    "\n",
    "bon_scores['bon_0'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_1'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_2'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_3'] = bon_scores['bon_3']/4\n",
    "\n",
    "bon_scores = bon_scores.reindex(columns=['bon_0', 'bon_1', 'bon_2', 'bon_3', 'bon_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB classification\n",
    "\n",
    "tweets = tweets['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pipeline\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "\n",
    "# load model\n",
    "\n",
    "nb = pickle.load( open( \"nb.model\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# define preprocess functions\n",
    "\n",
    "def remove_rt(x):\n",
    "    if 'RT @' in x:\n",
    "        try:\n",
    "            return x[x.find(':')+2:]\n",
    "        except:\n",
    "            return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop(x):\n",
    "    return [word for word in x.split() if word not in stop_words]\n",
    "\n",
    "\n",
    "# preprocess tweets\n",
    "\n",
    "tweets = tweets.apply(lambda x: remove_rt(x))\n",
    "tweets = tweets.apply(lambda x: re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x))\n",
    "tweets = tweets.apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "tweets = tweets.apply(lambda x: x.lower())\n",
    "tweets = tweets.apply(lambda x: remove_stop(x))\n",
    "tweets = tweets.astype(str)\n",
    "tweets = tweets[tweets!='[]']\n",
    "\n",
    "\n",
    "# perform predictions over tweets\n",
    "\n",
    "pred = nb.predict_proba(tweets)\n",
    "\n",
    "# return average of predictions\n",
    "\n",
    "nb_scores = np.mean(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06428571428571428,\n",
       " 0.06428571428571428,\n",
       " 0.06428571428571428,\n",
       " 0.06428571428571428,\n",
       " 0.7428571428571429]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR final prediction\n",
    "\n",
    "proba = pd.DataFrame(np.array(bon_scores)[0].tolist() + nb_scores.tolist() + rf_scores[0].tolist()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>0.677967</td>\n",
       "      <td>0.215173</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.080194</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.064286  0.064286  0.064286  0.064286  0.742857  0.023694  0.677967   \n",
       "\n",
       "         7         8         9    10   11   12   13   14  \n",
       "0  0.215173  0.002972  0.080194  0.1  0.0  0.5  0.2  0.2  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../scripts/lr.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005767441991484693, 0.23383913232371148, 0.25637080412642665, 0.029104485202460582, 0.4749181363559166]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict_proba(proba).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = proba * [1.19, 2.13, 0.96, 1, 1.12, 2.66, 0, 1.8, 2.88, 1.77]\n",
    "proba[0] = (proba[0] + proba[5]) / 2\n",
    "proba[1] = (proba[1] + proba[6]) / 2\n",
    "proba[2] = (proba[2] + proba[7]) / 2\n",
    "proba[3] = (proba[3] + proba[8]) / 2\n",
    "proba[4] = (proba[4] + proba[9]) / 2\n",
    "proba = proba.drop(columns=[5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27304885486418223,\n",
       " 0.36101736228004144,\n",
       " 0.6366415533218266,\n",
       " 0.6487430091919153,\n",
       " 0.19945440878089352]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba.iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../scripts/lr.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = []\n",
    "for j in range (0,9):\n",
    "    val = 0\n",
    "    for i in range (0,4):\n",
    "        val += lr.coef_[i][j]\n",
    "    coeff.append(val/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6733607956695952,\n",
       " -0.2811991430202746,\n",
       " -0.5576706205492699,\n",
       " 0.2853986450380394,\n",
       " -0.12494548246582574,\n",
       " -0.02120902048393063,\n",
       " 0.8545408071848849,\n",
       " 0.6892520659239165,\n",
       " -0.11664937459305129]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
