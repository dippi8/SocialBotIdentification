{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import tweepy\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "user = 'floriandaniel'\n",
    "\n",
    "\n",
    "# create connection with Twitter API\n",
    "\n",
    "CONSUMER_KEY = 'J3uhwWHdmSt3uD69ry8r2kc3B'\n",
    "CONSUMER_SECRET = 'LeuM7dVfFbXH0bc5fFkZIxlNiUXyjIx4Kcjh58HeWs2TrLijDx'\n",
    "ACCESS_TOKEN = '327497511-5dNjiGaTQHASljoxNr1qlMOlHrrSB21HBFBDjx2E'\n",
    "ACCESS_TOKEN_SECRET = 'tHLqwlhHYJWptW3femyq0rbMW6ZItu5yLQ3DFGJqJ8Xlg'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "\n",
    "# download user's tweets\n",
    "\n",
    "stuff = api.user_timeline(screen_name = user, count = 100, include_rts = True, tweet_mode=\"extended\")\n",
    "tweets = []\n",
    "\n",
    "for tweet in stuff:\n",
    "    tweet._json['user_id'] = tweet._json['user']['id']\n",
    "    if len(tweet._json['entities']['urls']) != 0:\n",
    "        tweet._json['url'] = tweet._json['entities']['urls'][0]['expanded_url']\n",
    "    else:\n",
    "         tweet._json['url'] = None\n",
    "    del tweet._json['user'], tweet._json['entities']\n",
    "\n",
    "    tweets.append(tweet._json)\n",
    "        \n",
    "tweets = pd.DataFrame.from_dict(tweets)\n",
    "\n",
    "\n",
    "# download user's features\n",
    "\n",
    "user = api.get_user(user)\n",
    "\n",
    "user_features = pd.DataFrame([[user.id,\n",
    "                    user.name,\n",
    "                    user.screen_name,\n",
    "                    user.statuses_count,\n",
    "                    user.followers_count,\n",
    "                    user.friends_count,\n",
    "                    user.favourites_count,\n",
    "                    user.listed_count,\n",
    "                    user.url,\n",
    "                    user.lang,\n",
    "                    user.time_zone,\n",
    "                    user.location,\n",
    "                    user.default_profile,\n",
    "                    user.default_profile_image,\n",
    "                    user.geo_enabled,\n",
    "                    user.profile_image_url,\n",
    "                    user.profile_use_background_image,\n",
    "                    user.profile_background_image_url_https,\n",
    "                    user.profile_text_color,\n",
    "                    user.profile_image_url_https,\n",
    "                    user.profile_sidebar_border_color,\n",
    "                    user.profile_background_tile,\n",
    "                    user.profile_sidebar_fill_color,\n",
    "                    user.profile_background_image_url,\n",
    "                    user.profile_background_color,\n",
    "                    user.profile_link_color,\n",
    "                    user.utc_offset,\n",
    "                    user.is_translator,\n",
    "                    user.follow_request_sent,\n",
    "                    user.protected,\n",
    "                    user.verified,\n",
    "                    user.notifications,\n",
    "                    user.description,\n",
    "                    user.contributors_enabled,\n",
    "                    user.following,\n",
    "                    user.created_at]],\n",
    "                    columns=[\"id\",\"name\",\"screen_name\",\"statuses_count\",\"followers_count\",\"friends_count\",\"favourites_count\",\"listed_count\",\"url\",\"lang\",\"time_zone\",\"location\",\"default_profile\",\"default_profile_image\",\"geo_enabled\",\"profile_image_url\",\"profile_use_background_image\",\"profile_background_image_url_https\",\"profile_text_color\",\"profile_image_url_https\",\"profile_sidebar_border_color\",\"profile_background_tile\",\"profile_sidebar_fill_color\",\"profile_background_image_url\",\"profile_background_color\",\"profile_link_color\",\"utc_offset\",\"is_translator\",\"follow_request_sent\",\"protected\",\"verified\",\"notifications\",\"description\",\"contributors_enabled\",\"following\",\"created_at\"]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context score\n",
    "\n",
    "porn_words = pd.read_csv('../scripts/data/porn/filtered_main_words.csv', sep=',')\n",
    "prop_words = pd.read_csv('../scripts/data/propaganda/filtered_main_words.csv', sep=',')\n",
    "spam_words = pd.read_csv('../scripts/data/spam/filtered_main_words.csv', sep=',')\n",
    "fake_words = pd.read_csv('../scripts/data/fake_followers/filtered_main_words.csv', sep=',')\n",
    "genuine_words = pd.read_csv('../scripts/data/genuine/filtered_main_words.csv', sep=',')\n",
    "\n",
    "def compute_score(tweets):\n",
    "\n",
    "    user_score = pd.DataFrame(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score'])\n",
    "\n",
    "    for tweet in tweets['full_text']:\n",
    "        # check for words in main_words and compute the scores for each tweet and for each category\n",
    "        mask = np.in1d(porn_words.word, tweet.split())\n",
    "        porn_score = porn_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(prop_words.word, tweet.split())\n",
    "        prop_score = prop_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(spam_words.word, tweet.split())\n",
    "        spam_score = spam_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(fake_words.word, tweet.split())\n",
    "        fake_score = fake_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(genuine_words.word, tweet.split())\n",
    "        genuine_score = genuine_words.loc[mask]['score'].values.sum()\n",
    "        \n",
    "        user_score = user_score.append(pd.DataFrame({'porn_words_score': porn_score, 'prop_words_score': prop_score, 'spam_words_score': spam_score,'fake_words_score': fake_score,'genuine_words_score': genuine_score}, index=[0]),sort=False, ignore_index=True)\n",
    "\n",
    "    return user_score\n",
    "\n",
    "\n",
    "if len(tweets) > 0:\n",
    "    # sum all the scores of each category\n",
    "    user_score = compute_score(tweets).sum()\n",
    "    scores = np.divide(user_score,len(tweets))\n",
    "else:\n",
    "    scores = pd.DataFrame({'porn_words_score': 0, 'prop_words_score': 0, 'spam_words_score': 0, 'fake_words_score': 0, 'genuine_words_score': 0}, index=[0]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute intradistances\n",
    "\n",
    "def compute_centroid(tf_idf):\n",
    "\n",
    "    center = tf_idf.sum(axis=1)/tf_idf.shape[0]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_centroid(tf_idf, centroid):\n",
    "    \n",
    "    distances = []\n",
    "    for elem in tf_idf:\n",
    "        distances.append(np.linalg.norm(tf_idf - centroid))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wss(id, tweets_df, is_tweet = 1):\n",
    "    \n",
    "    if is_tweet == 1:\n",
    "        # get tweets per id\n",
    "        vector = tweets_df['full_text']\n",
    "        n_vectors = len(vector)\n",
    "    elif is_tweet == 0:\n",
    "        # get domains per id\n",
    "        vector = tweets_df['url']\n",
    "        vector = vector.fillna('').astype(str)\n",
    "        for i in range(len(vector)):\n",
    "            vector.iloc[i] = urlparse(vector.iloc[i]).netloc\n",
    "        n_vectors = len(vector)\n",
    "    else:\n",
    "        print ('Invalid Input')\n",
    "\n",
    "    transformer = TfidfVectorizer(smooth_idf=True)\n",
    "    tf_idf = transformer.fit_transform(vector).todense()\n",
    "    \n",
    "    centroid = compute_centroid(tf_idf)\n",
    "    distances = dist_from_centroid(tf_idf, centroid)\n",
    "    avg_dist = np.asarray(distances).sum()/n_vectors\n",
    "    \n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intradistances():\n",
    "    try:\n",
    "        tw = (wss(user, tweets, 1))\n",
    "    except:\n",
    "        tw = 0\n",
    "        \n",
    "    try:\n",
    "        url = (wss(user, tweets, 0))\n",
    "    except:\n",
    "        url = 0\n",
    "        \n",
    "    return  pd.DataFrame({'tweets_intra': tw, 'url_intra': 0}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistances = intradistances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "from ast import literal_eval\n",
    "\n",
    "# Loads label file, strips off carriage return\n",
    "label_lines = [line.rstrip() for line \n",
    "                   in tf.gfile.GFile(\"../scripts/NSFW-detection/retrained_labels.txt\")]\n",
    "\n",
    "# Unpersists graph from file\n",
    "with tf.gfile.FastGFile(\"../scripts/NSFW-detection/retrained_graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "def nsfw(url):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, \"local-filename.jpg\")\n",
    "        image_path = 'local-filename.jpg'\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "        # Read in the image_data\n",
    "        image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # Feed the image_data as input to the graph and get first prediction\n",
    "            softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "            predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "            return predictions[0][1]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def nsfw_detection(bot_id):\n",
    "    \n",
    "    \n",
    "    porn = 0\n",
    "    tot = len(tweets.extended_entities[tweets.extended_entities.notnull()][:10])\n",
    "    \n",
    "    for media in tweets.extended_entities[tweets.extended_entities.notnull()][:10]:\n",
    "        try:\n",
    "            url = media['media'][0]['media_url_https']\n",
    "            if nsfw(url) > 0.5:\n",
    "                porn += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    if tot > 0:\n",
    "        nudity = porn/tot\n",
    "    else:\n",
    "        nudity = 0\n",
    "\n",
    "    try:\n",
    "        profile = user.profile_image_url_https.replace('normal', '400x400')\n",
    "        profile_nudity = nsfw(profile)\n",
    "    except:\n",
    "        profile_nudity = 0\n",
    "        \n",
    "        \n",
    "    return profile_nudity, nudity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'extended_entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-a30682ce2b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnsfw_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsfw_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsfw_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-f50aa2e3733f>\u001b[0m in \u001b[0;36mnsfw_detection\u001b[0;34m(bot_id)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mporn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_entities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmedia\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_entities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'extended_entities'"
     ]
    }
   ],
   "source": [
    "nsfw_profile, nsfw_avg = nsfw_detection(user.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'local-filename.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-1c08a2794979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local-filename.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'local-filename.jpg'"
     ]
    }
   ],
   "source": [
    "os.unlink('local-filename.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect descriptive features\n",
    "\n",
    "def describe_tweets(tweets):\n",
    "    \n",
    "    ret_perc, media_perc, url_perc, quote_perc = tweet_perc(tweets)\n",
    "    \n",
    "    avg_len, avg_ret, avg_fav, avg_hash = tweet_desc(tweets, 'avg')\n",
    "    max_len, max_ret, max_fav, max_hash = tweet_desc(tweets, 'max')\n",
    "    min_len, min_ret, min_fav, min_hash = tweet_desc(tweets, 'min')\n",
    "    \n",
    "    freq = tweet_freq(tweets)\n",
    "\n",
    "    frame = np.array([avg_len, max_len, min_len, avg_ret, max_ret, min_ret, avg_fav, max_fav, min_fav, avg_hash, max_hash, min_hash, freq, ret_perc, media_perc, url_perc, quote_perc])\n",
    "   \n",
    "    desc_features = pd.DataFrame({'avg_len': avg_len, 'max_len': max_len, 'min_len': min_len, 'avg_ret': avg_ret, 'max_ret': max_ret, 'min_ret': min_ret, 'avg_fav': avg_fav, 'max_fav': max_fav, 'min_fav': min_fav, 'avg_hash' : avg_hash, 'max_hash' : max_hash, 'min_hash' : max_hash,'freq': freq, 'ret_perc': ret_perc, 'media_perc': media_perc, 'url_perc': url_perc, 'quote_perc': quote_perc}, index=[0])\n",
    "    \n",
    "    return desc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_perc(tweets):\n",
    "    \n",
    "    try:\n",
    "        ret_perc = np.invert(tweets.retweeted_status.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        ret_perc = 0\n",
    "    try:\n",
    "        media_perc = np.invert(tweets.extended_entities.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        media_perc = 0\n",
    "    try:\n",
    "        url_perc = np.invert(tweets.url.isnull()).sum()/len(tweets)\n",
    "    except:\n",
    "        url_perc = 0\n",
    "    try:\n",
    "        quote_perc = tweets.is_quote_status.sum()/len(tweets)\n",
    "    except:\n",
    "        quote_perc = 0\n",
    "    \n",
    "    return ret_perc, media_perc, url_perc, quote_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_count(tweets):\n",
    "    \n",
    "    occurrences = []\n",
    "    for tweet in tweets:\n",
    "        occurrences.append(tweet.count('#'))\n",
    "        \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_desc(tweets, metric):\n",
    "    \n",
    "    tweets_lenght = tweets['full_text'].apply(lambda x: len(x))\n",
    "    \n",
    "    if metric == 'avg':\n",
    "        ret = np.mean(tweets.retweet_count)\n",
    "        lenght = np.mean(tweets_lenght)\n",
    "        fav = np.mean(tweets.favorite_count)\n",
    "        hashtag = np.mean(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'max':\n",
    "        ret = max(tweets.retweet_count)\n",
    "        lenght = max(tweets_lenght)\n",
    "        fav = max(tweets.favorite_count)\n",
    "        hashtag = max(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'min':\n",
    "        ret = min(tweets.retweet_count)\n",
    "        lenght = min(tweets_lenght)\n",
    "        fav = min(tweets.favorite_count)\n",
    "        hashtag = min(hashtag_count(tweets['full_text']))\n",
    "\n",
    "    return lenght, ret, fav, hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_freq(tweets):\n",
    "    \n",
    "    dates = list(tweets.created_at)\n",
    "    \n",
    "    last = dates[0]\n",
    "    d = last[8:10]\n",
    "    m = last[4:7]\n",
    "    y = last[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    last = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    first = dates[-1]\n",
    "    d = first[8:10]\n",
    "    m = first[4:7]\n",
    "    y = first[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    first = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    delta = (last - first).days + 1\n",
    "    freq = len(tweets)/delta\n",
    "    \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(tweets_df):\n",
    "    \n",
    "    tweets = tweets_df\n",
    "    \n",
    "    if len(tweets) > 0:\n",
    "        # sum all the scores of each category\n",
    "        features = describe_tweets(tweets)\n",
    "    else:\n",
    "        features = pd.DataFrame({'avg_len': 0, 'max_len': 0, 'min_len': 0,'avg_ret': 0, 'max_ret': 0, 'min_ret': 0, 'avg_fav': 0, 'max_fav': 0, 'min_fav': 0, 'avg_hash': 0, 'max_hash': 0, 'min_hash': 0, 'freq': 0, 'ret_perc': 0, 'media_perc': 0, 'url_perc': 0, 'quote_perc':0}, index=[0])\n",
    "    \n",
    "    # return the average scores of each user\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = describe(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([user_features, intradistances, scores, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF classification\n",
    "\n",
    "def oldness(x):\n",
    "    x = str(x)\n",
    "    if x[0] == '2':\n",
    "        return 2018 - int(x[:4])\n",
    "    else:\n",
    "        return 2018 - int(x[-4:])\n",
    "    \n",
    "full = full.drop(columns=['contributors_enabled', 'follow_request_sent', 'following', 'profile_background_image_url', 'profile_background_image_url_https', 'profile_image_url', 'profile_image_url_https', 'time_zone', 'utc_offset'])\n",
    "full = full.drop(columns=['default_profile_image','is_translator', 'geo_enabled', 'location', 'notifications', 'profile_background_tile', 'protected', 'verified'])\n",
    "full['default_profile'] = full['default_profile'].fillna(full['default_profile'].mode()[0])\n",
    "full['description'] = full['description'].fillna('')\n",
    "full['description_len'] = full['description'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['description'])\n",
    "full['name'] = full['name'].fillna('')\n",
    "full['name_len'] = full['name'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['name'])\n",
    "full['screen_name'] = full['screen_name'].fillna('')\n",
    "full['screen_name_len'] = full['screen_name'].apply(lambda x: len(x))\n",
    "full = full.drop(columns=['screen_name'])\n",
    "full = full.drop(columns=['lang'])\n",
    "full['age'] = full['created_at'].apply(lambda x: oldness(x))\n",
    "full = full.drop(columns=['created_at'])\n",
    "full = full.drop(columns=['id'])\n",
    "full['profile_use_background_image'] = full['profile_use_background_image'].fillna(full['profile_use_background_image'].mode()[0])\n",
    "full['url'] = (full['url'].notnull()).astype(int)\n",
    "full = full[['avg_fav', 'avg_hash', 'avg_len', 'avg_ret', 'default_profile',\n",
    "       'fake_words_score', 'favourites_count', 'followers_count', 'freq',\n",
    "       'friends_count', 'genuine_words_score', 'listed_count', 'max_fav',\n",
    "       'max_hash', 'max_len', 'max_ret', 'media_perc', 'min_fav', 'min_hash',\n",
    "       'min_len', 'min_ret', 'porn_words_score',\n",
    "       'profile_use_background_image', 'prop_words_score', 'quote_perc',\n",
    "       'ret_perc', 'spam_words_score', 'statuses_count',\n",
    "       'tweets_intra', 'url', 'url_intra', 'url_perc',\n",
    "       'description_len', 'name_len', 'screen_name_len', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['nsfw_profile'] = nsfw_profile\n",
    "full['nsfw_avg'] = nsfw_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pickle.load(open('../scripts/rf.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores = rf.predict_proba(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoN classification\n",
    "\n",
    "full.drop(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score', 'genuine_words_score', 'nsfw_profile', 'nsfw_avg'], inplace=True)\n",
    "\n",
    "# predict bot or not\n",
    "\n",
    "bon = pickle.load(open('../scripts/bot_or_not.model', 'rb'))\n",
    "bon_scores = bon.predict_proba(full)\n",
    "\n",
    "bon_scores = pd.DataFrame(bon_scores, columns=['bon_4', 'bon_3'])\n",
    "\n",
    "bon_scores['bon_0'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_1'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_2'] = bon_scores['bon_3']/4\n",
    "bon_scores['bon_3'] = bon_scores['bon_3']/4\n",
    "\n",
    "bon_scores = bon_scores.reindex(columns=['bon_0', 'bon_1', 'bon_2', 'bon_3', 'bon_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-5b3b3ae934a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NB classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_text'"
     ]
    }
   ],
   "source": [
    "# NB classification\n",
    "\n",
    "tweets = tweets['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 459948)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-99adfaf0c3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# perform predictions over tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# return average of predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1412\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 462\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 459948)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "# initialize pipeline\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "\n",
    "# load model\n",
    "\n",
    "nb = pickle.load( open( \"nb.model\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# define preprocess functions\n",
    "\n",
    "def remove_rt(x):\n",
    "    if 'RT @' in x:\n",
    "        try:\n",
    "            return x[x.find(':')+2:]\n",
    "        except:\n",
    "            return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop(x):\n",
    "    return [word for word in x.split() if word not in stop_words]\n",
    "\n",
    "\n",
    "# preprocess tweets\n",
    "\n",
    "tweets = tweets.apply(lambda x: remove_rt(x))\n",
    "tweets = tweets.apply(lambda x: re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x))\n",
    "tweets = tweets.apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "tweets = tweets.apply(lambda x: x.lower())\n",
    "tweets = tweets.apply(lambda x: remove_stop(x))\n",
    "tweets = tweets.astype(str)\n",
    "tweets = tweets[tweets!='[]']\n",
    "\n",
    "\n",
    "# perform predictions over tweets\n",
    "\n",
    "pred = nb.predict_proba(tweets)\n",
    "\n",
    "# return average of predictions\n",
    "\n",
    "nb_scores = np.mean(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scores = np.array([0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR final prediction\n",
    "\n",
    "proba = pd.DataFrame(np.array(bon_scores)[0].tolist() + nb_scores.tolist() + rf_scores[0].tolist()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4    5    6    7    8     9   \\\n",
       "0  0.157143  0.157143  0.157143  0.157143  0.371429  0.0  0.0  0.0  0.0  12.0   \n",
       "\n",
       "         10   11        12        13        14  \n",
       "0  0.566667  0.0  0.066667  0.333333  0.033333  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../scripts/lr.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30604046818882313, 2.2493113298313262e-10, 0.0892761276740334, 0.29852046592379794, 0.30616293798841443]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict_proba(proba).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 15: given 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-4fe246a3d47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproba\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.66\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.77\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Another DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[0;34m(left, right, axis)\u001b[0m\n\u001b[1;32m   1449\u001b[0m           not isinstance(right, (ABCSeries, ABCDataFrame))):\n\u001b[1;32m   1450\u001b[0m         \u001b[0;31m# GH17901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mto_series\u001b[0;34m(right)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 raise ValueError(msg.format(req_len=len(left.columns),\n\u001b[0;32m-> 1426\u001b[0;31m                                             given_len=len(right)))\n\u001b[0m\u001b[1;32m   1427\u001b[0m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to coerce to Series, length must be 15: given 10"
     ]
    }
   ],
   "source": [
    "proba = proba * [1.19, 2.13, 0.96, 1, 1.12, 2.66, 0, 1.8, 2.88, 1.77]\n",
    "proba[0] = (proba[0] + proba[5]) / 2\n",
    "proba[1] = (proba[1] + proba[6]) / 2\n",
    "proba[2] = (proba[2] + proba[7]) / 2\n",
    "proba[3] = (proba[3] + proba[8]) / 2\n",
    "proba[4] = (proba[4] + proba[9]) / 2\n",
    "proba = proba.drop(columns=[5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27304885486418223,\n",
       " 0.36101736228004144,\n",
       " 0.6366415533218266,\n",
       " 0.6487430091919153,\n",
       " 0.19945440878089352]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba.iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../scripts/lr.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = []\n",
    "for j in range (0,9):\n",
    "    val = 0\n",
    "    for i in range (0,4):\n",
    "        val += lr.coef_[i][j]\n",
    "    coeff.append(val/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6733607956695952,\n",
       " -0.2811991430202746,\n",
       " -0.5576706205492699,\n",
       " 0.2853986450380394,\n",
       " -0.12494548246582574,\n",
       " -0.02120902048393063,\n",
       " 0.8545408071848849,\n",
       " 0.6892520659239165,\n",
       " -0.11664937459305129]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
