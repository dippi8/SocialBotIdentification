{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets:\n",
    "#    0: porn\n",
    "#    1: propaganda\n",
    "#    2: spam\n",
    "#    3: fake followers\n",
    "#    4: genuine accounts\n",
    "\n",
    "porn_users = pd.read_csv('data/porn/users.csv', encoding='utf-8-sig')\n",
    "prop_users = pd.read_csv('data/propaganda/users.csv', encoding='utf-8-sig')\n",
    "spam_users = pd.read_csv('data/spam/users.csv', encoding='utf-8-sig')\n",
    "fake_users = pd.read_csv('data/fake_followers/users.csv', encoding='utf-8-sig')\n",
    "genuine_users = pd.read_csv('data/genuine/users.csv', encoding='utf-8-sig')\n",
    "\n",
    "porn_ids = porn_users['id']\n",
    "prop_ids = prop_users['id']\n",
    "spam_ids = spam_users['id']\n",
    "fake_ids = fake_users['id']\n",
    "gen_ids = genuine_users['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid(tf_idf):\n",
    "\n",
    "    center = tf_idf.sum(axis=1)/tf_idf.shape[0]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_centroid(tf_idf, centroid):\n",
    "    \n",
    "    distances = []\n",
    "    for elem in tf_idf:\n",
    "        distances.append(np.linalg.norm(tf_idf - centroid))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wss(id, tweets_df, is_tweet = 1):\n",
    "    \n",
    "    if is_tweet == 1:\n",
    "        # get tweets per id\n",
    "        vector = tweets_df[tweets_df.user_id == id]['full_text']\n",
    "        n_vectors = len(vector)\n",
    "    elif is_tweet == 0:\n",
    "        # get domains per id\n",
    "        vector = tweets_df[tweets_df.user_id == id]['url']\n",
    "        vector = vector.fillna('').astype(str)\n",
    "        for i in range(len(vector)):\n",
    "            vector.iloc[i] = urlparse(vector.iloc[i]).netloc\n",
    "        n_vectors = len(vector)\n",
    "    else:\n",
    "        print ('Invalid Input')\n",
    "\n",
    "    transformer = TfidfVectorizer(smooth_idf=True)\n",
    "    tf_idf = transformer.fit_transform(vector).todense()\n",
    "    \n",
    "    centroid = compute_centroid(tf_idf)\n",
    "    distances = dist_from_centroid(tf_idf, centroid)\n",
    "    avg_dist = np.asarray(distances).sum()/n_vectors\n",
    "    \n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intradistance(bot_ids, bot_type, is_tweet=1):\n",
    "    \n",
    "    tweets_df = pd.read_csv('data/' + bot_type + '/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "    \n",
    "    distances = []\n",
    "    i=0\n",
    "    for user in bot_ids:\n",
    "        i += 1\n",
    "        try:\n",
    "            distances.append(wss(user, tweets_df, is_tweet))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            distances.append(0)\n",
    "        if (i%10 == 0):\n",
    "            clear_output()\n",
    "            print(str(i) +  \" \" + bot_type + \" bots processed\")\n",
    "    \n",
    "    dist = pd.DataFrame()\n",
    "    dist['user_id'] = bot_ids.values\n",
    "    if is_tweet == 1:\n",
    "        dist['tweet_intradistance'] = distances\n",
    "        dist.to_csv('data/' + bot_type + '/tweet_intradistance.csv', index=False)\n",
    "    elif is_tweet == 0:\n",
    "        dist['url_intradistance'] = distances\n",
    "        dist.to_csv('data/' + bot_type + '/url_intradistance.csv', index=False)\n",
    "    else:\n",
    "        print ('Invalid Input')\n",
    "\n",
    "    print(bot_type + \" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1872a2b73d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mintradistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'porn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tweet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-de9765788383>\u001b[0m in \u001b[0;36mintradistance\u001b[0;34m(bot_ids, bot_type, is_tweet)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mintradistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbot_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tweet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbot_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tweets.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m     \"\"\"\n\u001b[1;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "intradistance(porn_ids, 'porn', is_tweet=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistance(porn_ids, 'porn', is_tweet=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistance(spam_ids, 'spam', is_tweet=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistance(spam_ids, 'spam', is_tweet=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intradistance(fake_ids, 'fake_followers', is_tweet=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9050 fake_followers bots processed\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "fake_followers done!\n"
     ]
    }
   ],
   "source": [
    "intradistance(fake_ids, 'fake_followers', is_tweet=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370 propaganda bots processed\n",
      "propaganda done!\n"
     ]
    }
   ],
   "source": [
    "intradistance(prop_ids, 'propaganda', is_tweet=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370 propaganda bots processed\n",
      "propaganda done!\n"
     ]
    }
   ],
   "source": [
    "intradistance(prop_ids, 'propaganda', is_tweet=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660 genuine bots processed\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "genuine done!\n"
     ]
    }
   ],
   "source": [
    "intradistance(gen_ids, 'genuine', is_tweet=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660 genuine bots processed\n",
      "empty vocabulary; perhaps the documents only contain stop words\n",
      "genuine done!\n"
     ]
    }
   ],
   "source": [
    "intradistance(gen_ids, 'genuine', is_tweet=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,8,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,4,8,11,18,19,20,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,8,31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "\n",
    "porn_tweets_df = pd.read_csv('data/porn/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "prop_tweets_df = pd.read_csv('data/propaganda/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "spam_tweets_df = pd.read_csv('data/spam/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "fake_tweets_df = pd.read_csv('data/fake_followers/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "genuine_tweets_df = pd.read_csv('data/genuine/tweets.csv', encoding='utf-8-sig', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def get_main_words(tweets_df):\n",
    "    \n",
    "    n_words = 300\n",
    "    tweets = tweets_df['full_text'].values.astype('str')\n",
    "    \n",
    "    #tokenize and remove stop words \n",
    "    punctuation = list(string.punctuation)\n",
    "    stopWords = stopwords.words('english') + stopwords.words('italian') + stopwords.words('french') + stopwords.words('spanish') + punctuation + ['...', '\"the', \"i'm\", 'go', 'time', 'get', 'rt', 'via', '&amp;'] + [\"it's\"]\n",
    "\n",
    "    word_counter = Counter()\n",
    "    for elem in tweets:\n",
    "        word_counter.update(elem.lower().split())\n",
    "    \n",
    "    for word in stopWords:\n",
    "        if word in word_counter:\n",
    "            del word_counter[word]\n",
    "            \n",
    "\n",
    "    main_words = pd.DataFrame(data=word_counter.most_common(n_words), index=None, columns=['word', 'score'])\n",
    "    \n",
    "    return main_words[:n_words-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vocabularies():\n",
    "    porn_voc = pd.read_csv('data/porn/main_words.csv', sep=',')\n",
    "    prop_voc = pd.read_csv('data/propaganda/main_words.csv', sep=',')\n",
    "    spam_voc = pd.read_csv('data/spam/main_words.csv', sep=',')\n",
    "    fake_voc = pd.read_csv('data/fake_followers/main_words.csv', sep=',')\n",
    "    genuine_voc = pd.read_csv('data/genuine/main_words.csv', sep=',')\n",
    "    \n",
    "    porn_words = set(porn_voc['word'])\n",
    "    porn_main_words = set(porn_voc['word'][:50])\n",
    "    prop_words = set(prop_voc['word'])\n",
    "    prop_main_words = set(prop_voc['word'][:50])\n",
    "    spam_words = set(spam_voc['word'])\n",
    "    spam_main_words = set(spam_voc['word'][:50])\n",
    "    fake_words = set(fake_voc['word'])\n",
    "    fake_main_words = set(fake_voc['word'][:50])\n",
    "    genuine_words = set(genuine_voc['word'])\n",
    "    genuine_main_words = set(genuine_voc['word'][:50])\n",
    "    \n",
    "    new_porn_words = porn_words - set(prop_main_words | spam_main_words | fake_main_words | genuine_main_words)\n",
    "    new_prop_words = prop_words - set(porn_main_words | spam_main_words | fake_main_words | genuine_main_words)\n",
    "    new_spam_words = spam_words - set(prop_main_words | porn_main_words | fake_main_words | genuine_main_words)\n",
    "    new_fake_words = fake_words - set(prop_main_words | spam_main_words | porn_main_words | genuine_main_words)\n",
    "    new_genuine_words = genuine_words - set(prop_main_words | spam_main_words | porn_main_words | fake_main_words)\n",
    "    \n",
    "    print(str(len(new_porn_words)) + ' porn words')\n",
    "    print(str(len(new_prop_words)) + ' prop words')\n",
    "    print(str(len(new_spam_words)) + ' spam words')\n",
    "    print(str(len(new_fake_words)) + ' fake words')\n",
    "    print(str(len(new_genuine_words)) + ' genuine words')\n",
    "    \n",
    "    # normalize scores\n",
    "    porn_voc = porn_voc[porn_voc['word'].isin(list(new_porn_words))]\n",
    "    scaler = MinMaxScaler() \n",
    "    porn_voc['score'] = scaler.fit_transform(porn_voc.score.values.reshape(-1, 1))\n",
    "    porn_voc.drop(columns='Unnamed: 0', inplace=True)\n",
    "    porn_voc.drop(porn_voc.tail(1).index, inplace=True)\n",
    "    porn_voc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    prop_voc = prop_voc[prop_voc['word'].isin(list(new_prop_words))]\n",
    "    scaler = MinMaxScaler()\n",
    "    prop_voc['score'] = scaler.fit_transform(prop_voc.score.values.reshape(-1, 1))\n",
    "    prop_voc.drop(columns='Unnamed: 0', inplace=True)\n",
    "    prop_voc.drop(prop_voc.tail(1).index, inplace=True)\n",
    "    prop_voc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    spam_voc = spam_voc[spam_voc['word'].isin(list(new_spam_words))]\n",
    "    scaler = MinMaxScaler()\n",
    "    spam_voc['score'] = scaler.fit_transform(spam_voc.score.values.reshape(-1, 1))\n",
    "    spam_voc.drop(columns='Unnamed: 0', inplace=True)\n",
    "    spam_voc.drop(spam_voc.tail(1).index, inplace=True)\n",
    "    spam_voc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    fake_voc = fake_voc[fake_voc['word'].isin(list(new_fake_words))]\n",
    "    scaler = MinMaxScaler()\n",
    "    fake_voc['score'] = scaler.fit_transform(fake_voc.score.values.reshape(-1, 1))\n",
    "    fake_voc.drop(columns='Unnamed: 0', inplace=True)\n",
    "    fake_voc.drop(fake_voc.tail(1).index, inplace=True)\n",
    "    fake_voc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    genuine_voc = genuine_voc[genuine_voc['word'].isin(list(new_genuine_words))]\n",
    "    scaler = MinMaxScaler()\n",
    "    genuine_voc['score'] = scaler.fit_transform(genuine_voc.score.values.reshape(-1, 1))\n",
    "    genuine_voc.drop(columns='Unnamed: 0', inplace=True)\n",
    "    genuine_voc.drop(genuine_voc.tail(1).index, inplace=True)\n",
    "    genuine_voc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    porn_voc.to_csv('data/porn/filtered_main_words.csv', encoding='utf-8-sig')\n",
    "    prop_voc.to_csv('data/propaganda/filtered_main_words.csv', encoding='utf-8-sig')\n",
    "    spam_voc.to_csv('data/spam/filtered_main_words.csv', encoding='utf-8-sig')\n",
    "    fake_voc.to_csv('data/fake_followers/filtered_main_words.csv', encoding='utf-8-sig')\n",
    "    genuine_voc.to_csv('data/genuine/filtered_main_words.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "porn_vocabulary = get_main_words(porn_tweets_df)\n",
    "porn_vocabulary.to_csv('data/porn/main_words.csv', encoding='utf-8-sig')\n",
    "prop_vocabulary = get_main_words(prop_tweets_df)\n",
    "prop_vocabulary.to_csv('data/propaganda/main_words.csv', encoding='utf-8-sig')\n",
    "spam_vocabulary = get_main_words(spam_tweets_df)\n",
    "spam_vocabulary.to_csv('data/spam/main_words.csv', encoding='utf-8-sig')\n",
    "fake_vocabulary = get_main_words(fake_tweets_df)\n",
    "fake_vocabulary.to_csv('data/fake_followers/main_words.csv', encoding='utf-8-sig')\n",
    "genuine_vocabulary = get_main_words(genuine_tweets_df)\n",
    "genuine_vocabulary.to_csv('data/genuine/main_words.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 porn words\n",
      "238 prop words\n",
      "231 spam words\n",
      "234 fake words\n",
      "224 genuine words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "unique_vocabularies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Context Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "porn_words = pd.read_csv('data/porn/filtered_main_words.csv', sep=',')\n",
    "prop_words = pd.read_csv('data/propaganda/filtered_main_words.csv', sep=',')\n",
    "spam_words = pd.read_csv('data/spam/filtered_main_words.csv', sep=',')\n",
    "fake_words = pd.read_csv('data/fake_followers/filtered_main_words.csv', sep=',')\n",
    "genuine_words = pd.read_csv('data/genuine/filtered_main_words.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(tweets):\n",
    "\n",
    "    user_score = pd.DataFrame(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score'])\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # check for words in main_words and compute the scores for each tweet and for each category\n",
    "        mask = np.in1d(porn_words.word, tweet.split())\n",
    "        porn_score = porn_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(prop_words.word, tweet.split())\n",
    "        prop_score = prop_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(spam_words.word, tweet.split())\n",
    "        spam_score = spam_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(fake_words.word, tweet.split())\n",
    "        fake_score = fake_words.loc[mask]['score'].values.sum()\n",
    "        mask = np.in1d(genuine_words.word, tweet.split())\n",
    "        genuine_score = genuine_words.loc[mask]['score'].values.sum()\n",
    "        \n",
    "        user_score = user_score.append(pd.DataFrame({'porn_words_score': porn_score, 'prop_words_score': prop_score, 'spam_words_score': spam_score,'fake_words_score': fake_score,'genuine_words_score': genuine_score}, index=[0]), ignore_index=True)\n",
    "\n",
    "    return user_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(tweets_df, id):\n",
    "    \n",
    "    tweets = tweets_df[tweets_df.user_id == id]['full_text']\n",
    "    if len(tweets) > 0:\n",
    "        # sum all the scores of each category\n",
    "        user_score = compute_score(tweets).sum()\n",
    "        scores = np.divide(user_score,len(tweets))\n",
    "    else:\n",
    "        scores = pd.DataFrame({'porn_words_score': 0, 'prop_words_score': 0, 'spam_words_score': 0, 'fake_words_score': 0, 'genuine_words_score': 0}, index=[0])\n",
    "    \n",
    "    # return the average scores of each user\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_score(bot_ids, bot_type):\n",
    "    \n",
    "    tweets_df = pd.read_csv('data/' + bot_type + '/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "        \n",
    "    score_df = pd.DataFrame(columns=['porn_words_score', 'prop_words_score', 'spam_words_score', 'fake_words_score', 'genuine_words_score', 'user_id'])\n",
    "    i = 0\n",
    "    for user_id in bot_ids:\n",
    "        i += 1\n",
    "        scores = score(tweets_df, user_id)\n",
    "        score_df = score_df.append(scores, ignore_index=True)\n",
    "        if (i%10 == 0):\n",
    "            clear_output()\n",
    "            print(str(i) +  \" \" + bot_type + \" bots processed\")\n",
    "    \n",
    "    score_df['user_id'] = bot_ids.values\n",
    "    \n",
    "    score_df.reset_index(drop=True, inplace=True)\n",
    "    score_df.to_csv('data/' + bot_type + '/context_score.csv', index=False)\n",
    "\n",
    "    print(bot_type + \" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6930 porn bots processed\n",
      "porn done!\n"
     ]
    }
   ],
   "source": [
    "context_score(porn_ids, 'porn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370 propaganda bots processed\n",
      "propaganda done!\n"
     ]
    }
   ],
   "source": [
    "context_score(prop_ids, 'propaganda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5360 spam bots processed\n",
      "spam done!\n"
     ]
    }
   ],
   "source": [
    "context_score(spam_ids, 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9050 fake_followers bots processed\n",
      "fake_followers done!\n"
     ]
    }
   ],
   "source": [
    "context_score(fake_ids, 'fake_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660 genuine bots processed\n",
      "genuine done!\n"
     ]
    }
   ],
   "source": [
    "context_score(gen_ids, 'genuine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_tweets(tweets):\n",
    "    \n",
    "    ret_perc, media_perc, url_perc, quote_perc = tweet_perc(tweets)\n",
    "    \n",
    "    avg_len, avg_ret, avg_fav, avg_hash = tweet_desc(tweets, 'avg')\n",
    "    max_len, max_ret, max_fav, max_hash = tweet_desc(tweets, 'max')\n",
    "    min_len, min_ret, min_fav, min_hash = tweet_desc(tweets, 'min')\n",
    "    \n",
    "    freq = tweet_freq(tweets)\n",
    "\n",
    "    frame = np.array([avg_len, max_len, min_len, avg_ret, max_ret, min_ret, avg_fav, max_fav, min_fav, avg_hash, max_hash, min_hash, freq, ret_perc, media_perc, url_perc, quote_perc])\n",
    "   \n",
    "    desc_features = pd.DataFrame({'avg_len': avg_len, 'max_len': max_len, 'min_len': min_len, 'avg_ret': avg_ret, 'max_ret': max_ret, 'min_ret': min_ret, 'avg_fav': avg_fav, 'max_fav': max_fav, 'min_fav': min_fav, 'avg_hash' : avg_hash, 'max_hash' : max_hash, 'min_hash' : max_hash,'freq': freq, 'ret_perc': ret_perc, 'media_perc': media_perc, 'url_perc': url_perc, 'quote_perc': quote_perc}, index=[0])\n",
    "    \n",
    "    return desc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_perc(tweets):\n",
    "    \n",
    "    ret_perc = np.invert(tweets.retweeted_status.isnull()).sum()/len(tweets)\n",
    "    media_perc = np.invert(tweets.extended_entities.isnull()).sum()/len(tweets)\n",
    "    url_perc = np.invert(tweets.url.isnull()).sum()/len(tweets)\n",
    "    quote_perc = tweets.is_quote_status.sum()/len(tweets)\n",
    "    \n",
    "    return ret_perc, media_perc, url_perc, quote_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_count(tweets):\n",
    "    \n",
    "    occurrences = []\n",
    "    for tweet in tweets:\n",
    "        occurrences.append(tweet.count('#'))\n",
    "        \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_desc(tweets, metric):\n",
    "    \n",
    "    tweets_lenght = tweets['full_text'].apply(lambda x: len(x))\n",
    "    \n",
    "    if metric == 'avg':\n",
    "        ret = np.mean(tweets.retweet_count)\n",
    "        lenght = np.mean(tweets_lenght)\n",
    "        fav = np.mean(tweets.favorite_count)\n",
    "        hashtag = np.mean(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'max':\n",
    "        ret = max(tweets.retweet_count)\n",
    "        lenght = max(tweets_lenght)\n",
    "        fav = max(tweets.favorite_count)\n",
    "        hashtag = max(hashtag_count(tweets['full_text']))\n",
    "    elif metric == 'min':\n",
    "        ret = min(tweets.retweet_count)\n",
    "        lenght = min(tweets_lenght)\n",
    "        fav = min(tweets.favorite_count)\n",
    "        hashtag = min(hashtag_count(tweets['full_text']))\n",
    "\n",
    "    return lenght, ret, fav, hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_freq(tweets):\n",
    "    \n",
    "    dates = list(tweets.created_at)\n",
    "    \n",
    "    last = dates[0]\n",
    "    d = last[8:10]\n",
    "    m = last[4:7]\n",
    "    y = last[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    last = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    first = dates[-1]\n",
    "    d = first[8:10]\n",
    "    m = first[4:7]\n",
    "    y = first[-4:]\n",
    "    date = d + ' ' + m + ' ' + y\n",
    "    first = datetime.strptime(date, '%d %b %Y')\n",
    "    \n",
    "    delta = (last - first).days + 1\n",
    "    freq = len(tweets)/delta\n",
    "    \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(tweets_df, id):\n",
    "    \n",
    "    tweets = tweets_df[tweets_df.user_id == id]\n",
    "    \n",
    "    if len(tweets) > 0:\n",
    "        # sum all the scores of each category\n",
    "        features = describe_tweets(tweets)\n",
    "    else:\n",
    "        features = pd.DataFrame({'avg_len': 0, 'max_len': 0, 'min_len': 0,'avg_ret': 0, 'max_ret': 0, 'min_ret': 0, 'avg_fav': 0, 'max_fav': 0, 'min_fav': 0, 'avg_hash': 0, 'max_hash': 0, 'min_hash': 0, 'freq': 0, 'ret_perc': 0, 'media_perc': 0, 'url_perc': 0, 'quote_perc':0}, index=[0])\n",
    "    \n",
    "    # return the average scores of each user\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_features(bot_ids, bot_type):\n",
    "    \n",
    "    tweets_df = pd.read_csv('data/' + bot_type + '/tweets.csv', encoding='utf-8-sig', sep='\\t')\n",
    "        \n",
    "    desc_df = pd.DataFrame(columns=['avg_len', 'max_len', 'min_len', 'avg_ret', 'max_ret', 'min_ret', 'avg_fav', 'max_fav', 'min_fav', 'avg_hash', 'max_hash', 'min_hash', 'freq', 'ret_perc', 'media_perc', 'url_perc', 'quote_perc'])\n",
    "    i = 0\n",
    "    \n",
    "    for user_id in bot_ids:\n",
    "        i += 1\n",
    "        features = describe(tweets_df, user_id)\n",
    "        desc_df = desc_df.append(features, ignore_index=True)\n",
    "        if (i%10 == 0):\n",
    "            clear_output()\n",
    "            print(str(i) +  \" \" + bot_type + \" bots processed\")\n",
    "    \n",
    "    desc_df['user_id'] = bot_ids.values\n",
    "    \n",
    "    desc_df.reset_index(drop=True, inplace=True)\n",
    "    desc_df.to_csv('data/' + bot_type + '/descriptive_features.csv', index=False)\n",
    "\n",
    "    print(bot_type + \" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6930 porn bots processed\n",
      "porn done!\n"
     ]
    }
   ],
   "source": [
    "descriptive_features(porn_ids, 'porn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370 propaganda bots processed\n",
      "propaganda done!\n"
     ]
    }
   ],
   "source": [
    "descriptive_features(prop_ids, 'propaganda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5360 spam bots processed\n",
      "spam done!\n"
     ]
    }
   ],
   "source": [
    "descriptive_features(spam_ids, 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9050 fake_followers bots processed\n",
      "fake_followers done!\n"
     ]
    }
   ],
   "source": [
    "descriptive_features(fake_ids, 'fake_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660 genuine bots processed\n",
      "genuine done!\n"
     ]
    }
   ],
   "source": [
    "descriptive_features(gen_ids, 'genuine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize_tweets(tweets_df, k=5):\n",
    "    \n",
    "    tweets = tweets_df['full_text']\n",
    "    \n",
    "    transformer = TfidfVectorizer(smooth_idf=True)\n",
    "    tf_idf = transformer.fit_transform(tweets).todense()\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(tf_idf)\n",
    "    \n",
    "    return kmeans.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lorenzo/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,8,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('data/porn/tweets.csv', encoding='utf-8-sig', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterize_tweets(tweets_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2253931df11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/prop/tweets.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('data/prop/tweets.csv', encoding='utf-8-sig', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a1df294b701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
