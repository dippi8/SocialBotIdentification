{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import tweepy\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from flask import Flask, request, render_template, jsonify, current_app\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "from ast import literal_eval\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# create connection with Twitter API\n",
    "\n",
    "CONSUMER_KEY = 'GeWIh6LTIiPd2mgMh4PhesKkX'\n",
    "CONSUMER_SECRET = 'BjfHQp3T6XY3esXtYWo7mEh58zUd4v7USruqT1brFTXC0qOUYF'\n",
    "ACCESS_TOKEN = '973862126216499200-tr3zhK2hOHnnhqegZMviFpoUj3T9nHa'\n",
    "ACCESS_TOKEN_SECRET = 'LuHRMgpoGT3kjR95z8XPuafllrCaWRHyu86Pdru4zdATD'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users1 = pickle.load(open('ids_1.list','rb'))\n",
    "users2 = pickle.load(open('ids_2.list','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users1.tolist() + users2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matteo/Scrivania/script\n"
     ]
    }
   ],
   "source": [
    "cd ../../../../script/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-31a82e9ac519>:17: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "\n",
    "def create_labels():\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line \n",
    "                       in tf.gfile.GFile(\"models/NSFW-detection/retrained_labels.txt\")]\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(\"models/NSFW-detection/retrained_graph.pb\", 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "create_labels()\n",
    "create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(user):\n",
    "\n",
    "    # download user's tweets\n",
    "\n",
    "    stuff = api.user_timeline(user_id = user, count = 100, include_rts = True, tweet_mode=\"extended\")\n",
    "    tweets = []\n",
    "\n",
    "    for tweet in stuff:\n",
    "        tweet._json['user_id'] = tweet._json['user']['id']\n",
    "        if len(tweet._json['entities']['urls']) != 0:\n",
    "            tweet._json['url'] = tweet._json['entities']['urls'][0]['expanded_url']\n",
    "        else:\n",
    "             tweet._json['url'] = None\n",
    "        del tweet._json['user'], tweet._json['entities']\n",
    "\n",
    "        tweets.append(tweet._json)\n",
    "        \n",
    "    tweets = pd.DataFrame.from_dict(tweets)\n",
    "\n",
    "\n",
    "    # download user's features\n",
    "\n",
    "    user = api.get_user(user)\n",
    "\n",
    "    user_features = pd.DataFrame([[user.id,\n",
    "                        user.name,\n",
    "                        user.screen_name,\n",
    "                        user.statuses_count,\n",
    "                        user.followers_count,\n",
    "                        user.friends_count,\n",
    "                        user.favourites_count,\n",
    "                        user.listed_count,\n",
    "                        user.url,\n",
    "                        user.lang,\n",
    "                        user.time_zone,\n",
    "                        user.location,\n",
    "                        user.default_profile,\n",
    "                        user.default_profile_image,\n",
    "                        user.geo_enabled,\n",
    "                        user.profile_image_url,\n",
    "                        user.profile_use_background_image,\n",
    "                        user.profile_background_image_url_https,\n",
    "                        user.profile_text_color,\n",
    "                        user.profile_image_url_https,\n",
    "                        user.profile_sidebar_border_color,\n",
    "                        user.profile_background_tile,\n",
    "                        user.profile_sidebar_fill_color,\n",
    "                        user.profile_background_image_url,\n",
    "                        user.profile_background_color,\n",
    "                        user.profile_link_color,\n",
    "                        user.utc_offset,\n",
    "                        user.is_translator,\n",
    "                        user.follow_request_sent,\n",
    "                        user.protected,\n",
    "                        user.verified,\n",
    "                        user.notifications,\n",
    "                        user.description,\n",
    "                        user.contributors_enabled,\n",
    "                        user.following,\n",
    "                        user.created_at]],\n",
    "                        columns=[\"id\",\"name\",\"screen_name\",\"statuses_count\",\"followers_count\",\"friends_count\",\"favourites_count\",\"listed_count\",\"url\",\"lang\",\"time_zone\",\"location\",\"default_profile\",\"default_profile_image\",\"geo_enabled\",\"profile_image_url\",\"profile_use_background_image\",\"profile_background_image_url_https\",\"profile_text_color\",\"profile_image_url_https\",\"profile_sidebar_border_color\",\"profile_background_tile\",\"profile_sidebar_fill_color\",\"profile_background_image_url\",\"profile_background_color\",\"profile_link_color\",\"utc_offset\",\"is_translator\",\"follow_request_sent\",\"protected\",\"verified\",\"notifications\",\"description\",\"contributors_enabled\",\"following\",\"created_at\"]\n",
    "                      )\n",
    "    # Check if user is classifyble\n",
    "    if user.statuses_count == 0 and user.friends_count == 0 and user.followers_count == 0 and user.listed_count == 0 and user.favourites_count == 0:\n",
    "        return [0,0,0,0,100]\n",
    "\n",
    "    # Check if user is protected\n",
    "    if user.protected == True:\n",
    "        return [12.5,12.5,12.5,12.5,50]\n",
    "    # download user's tweets\n",
    "    stuff = api.user_timeline(user_id = user, count = 100, include_rts = True, tweet_mode=\"extended\")\n",
    "    tweets = []\n",
    "\n",
    "    for tweet in stuff:\n",
    "        tweet._json['user_id'] = tweet._json['user']['id']\n",
    "        if len(tweet._json['entities']['urls']) != 0:\n",
    "            tweet._json['url'] = tweet._json['entities']['urls'][0]['expanded_url']\n",
    "        else:\n",
    "             tweet._json['url'] = None\n",
    "        del tweet._json['user'], tweet._json['entities']\n",
    "\n",
    "        tweets.append(tweet._json)\n",
    "\n",
    "    tweets = pd.DataFrame.from_dict(tweets)\n",
    "\n",
    "    nb = pickle.load(open('models/nb.model', 'rb'))\n",
    "    bon = pickle.load(open('models/bot_or_not.model', 'rb'))\n",
    "    rf = pickle.load(open('models/rf.model', 'rb'))\n",
    "    lr = pickle.load(open('models/lr.model', 'rb'))\n",
    "    knn = pickle.load(open('models/knn.model', 'rb'))\n",
    "    knn_weights = pickle.load(open('models/knn.weights', 'rb'))\n",
    "\n",
    "\n",
    "    # compute context score\n",
    "\n",
    "    porn_words = pd.read_csv('data/nsfw/filtered_main_words.csv', sep=',')\n",
    "    prop_words = pd.read_csv('data/propaganda/filtered_main_words.csv', sep=',')\n",
    "    spam_words = pd.read_csv('data/spam/filtered_main_words.csv', sep=',')\n",
    "    fake_words = pd.read_csv('data/fake_followers/filtered_main_words.csv', sep=',')\n",
    "\n",
    "    bots_words = pd.read_csv('data/bots/filtered_main_words.csv', sep=',')\n",
    "    gen_words = pd.read_csv('data/genuine/filtered_main_words.csv', sep=',')\n",
    "\n",
    "    def compute_score(tweets):\n",
    "\n",
    "        user_score = pd.DataFrame(columns=['NSFW_words_score','news_spreaders_words_score','spam_bots_words_score','fake_followers_words_score','bot_words_score','gen_words_score'])\n",
    "\n",
    "        for tweet in tweets['full_text']:\n",
    "            # check for words in main_words and compute the scores for each tweet and for each category\n",
    "            mask = np.in1d(porn_words.word, tweet.split())\n",
    "            porn_score = porn_words.loc[mask]['score'].values.sum()\n",
    "            mask = np.in1d(prop_words.word, tweet.split())\n",
    "            prop_score = prop_words.loc[mask]['score'].values.sum()\n",
    "            mask = np.in1d(spam_words.word, tweet.split())\n",
    "            spam_score = spam_words.loc[mask]['score'].values.sum()\n",
    "            mask = np.in1d(fake_words.word, tweet.split())\n",
    "            fake_score = fake_words.loc[mask]['score'].values.sum()\n",
    "            mask = np.in1d(gen_words.word, tweet.split())\n",
    "            genuine_score = gen_words.loc[mask]['score'].values.sum()\n",
    "            mask = np.in1d(bots_words.word, tweet.split())\n",
    "            bots_score = bots_words.loc[mask]['score'].values.sum()\n",
    "\n",
    "\n",
    "            user_score = user_score.append(pd.DataFrame({'NSFW_words_score': porn_score, 'news_spreaders_words_score': prop_score, 'spam_bots_words_score': spam_score,'fake_followers_words_score': fake_score, 'bot_words_score':bots_score, 'gen_words_score':genuine_score}, index=[0]), sort=False, ignore_index=True)\n",
    "\n",
    "        return user_score\n",
    "\n",
    "    if len(tweets) > 0:\n",
    "        # sum all the scores of each category\n",
    "        user_score = compute_score(tweets).sum()\n",
    "        scores = np.divide(user_score,len(tweets))\n",
    "    else:\n",
    "        scores = pd.DataFrame({'NSFW_words_score': 0, 'news_spreaders_words_score': 0, 'spam_bots_words_score': 0, 'fake_followers_words_score': 0, 'bot_words_score':0, 'gen_words_score':0}, index=[0]).T\n",
    "\n",
    "    scores = pd.DataFrame(scores).T\n",
    "\n",
    "\n",
    "    # compute intradistances\n",
    "    def compute_centroid(tf_idf):\n",
    "\n",
    "        center = tf_idf.sum(axis=1)/tf_idf.shape[0]\n",
    "        return center\n",
    "\n",
    "\n",
    "    def dist_from_centroid(tf_idf, centroid):\n",
    "\n",
    "        distances = []\n",
    "        for elem in tf_idf:\n",
    "            distances.append(np.linalg.norm(tf_idf - centroid))\n",
    "        return distances\n",
    "\n",
    "\n",
    "    def wss(id, tweets_df, is_tweet = 1):\n",
    "\n",
    "        if is_tweet == 1:\n",
    "            # get tweets per id\n",
    "            vector = tweets_df['full_text']\n",
    "            n_vectors = len(vector)\n",
    "        elif is_tweet == 0:\n",
    "            # get domains per id\n",
    "            vector = tweets_df['url']\n",
    "            vector = vector.fillna('').astype(str)\n",
    "            for i in range(len(vector)):\n",
    "                vector.iloc[i] = urlparse(vector.iloc[i]).netloc\n",
    "            n_vectors = len(vector)\n",
    "        else:\n",
    "            print ('Invalid Input')\n",
    "\n",
    "        transformer = TfidfVectorizer(smooth_idf=True)\n",
    "        tf_idf = transformer.fit_transform(vector).todense()\n",
    "\n",
    "        centroid = compute_centroid(tf_idf)\n",
    "        distances = dist_from_centroid(tf_idf, centroid)\n",
    "        avg_dist = np.asarray(distances).sum()/n_vectors\n",
    "\n",
    "        return avg_dist\n",
    "\n",
    "\n",
    "    def intradistances():\n",
    "        try:\n",
    "            tw = (wss(user, tweets, 1))\n",
    "        except:\n",
    "            tw = 0\n",
    "        return  tw\n",
    "\n",
    "\n",
    "    intradistance = intradistances()\n",
    "\n",
    "    def get_url(df):\n",
    "\n",
    "        expanded_urls = []\n",
    "        for x in df.itertuples():\n",
    "            try:\n",
    "                if len(x.url)>0:\n",
    "                    expanded_urls.append(urlparse(x.url).netloc)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        expanded_urls = list(map(lambda x: x.replace('www.','').replace('/',''), expanded_urls))\n",
    "        return expanded_urls\n",
    "\n",
    "    import collections\n",
    "\n",
    "    def compute_entropy():\n",
    "\n",
    "        #urls = tweets['url']\n",
    "        #is_quote = tweets['is_quote_status']\n",
    "        if len(tweets) > 0:\n",
    "            param = tweets[['url','is_quote_status']]\n",
    "\n",
    "            if len(param) > 0:\n",
    "                urls = get_url(param)\n",
    "\n",
    "                # remove empty strings\n",
    "                urls = [x for x in urls if x]\n",
    "\n",
    "                if len(urls) > 0:\n",
    "                    # count frequency\n",
    "                    counter=collections.Counter(urls)\n",
    "                    occurrences = np.array(list(counter.values()))\n",
    "\n",
    "                    # update N\n",
    "                    N = occurrences.sum()\n",
    "\n",
    "                    p = occurrences/N\n",
    "                    entropies = -p *np.log2(p)\n",
    "                    entropy = entropies.sum()\n",
    "                else:\n",
    "                    entropy = 0.0\n",
    "            else:\n",
    "                entropy = 0.0\n",
    "\n",
    "        else:\n",
    "            entropy = 0.0\n",
    "\n",
    "        return float(entropy)\n",
    "\n",
    "\n",
    "    url_entropy = compute_entropy()\n",
    "\n",
    "    intradistances = pd.DataFrame({'url_entropy': url_entropy, 'tweet_intradistance': intradistance},index=[0])\n",
    "\n",
    "    # compute unreliability rate\n",
    "    #domains = pickle.load(open('data/fake_url.sources','rb'))\n",
    "    #urls = get_url(tweets)\n",
    "    #unreliability_rate = np.in1d(urls, domains).astype(int).sum() / len(urls)\n",
    "\n",
    "\n",
    "\n",
    "    def nsfw(url, sess):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, \"local-filename.jpg\")\n",
    "            image_path = 'local-filename.jpg'\n",
    "            os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "            # Read in the image_data\n",
    "            image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "\n",
    "            # Feed the image_data as input to the graph and get first prediction\n",
    "            softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "            predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "            return predictions[0][1]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def nsfw_detection(bot_id):\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            porn, tot = 0, 0\n",
    "            try:\n",
    "                for media in tweets.extended_entities[tweets.extended_entities.notnull()][:10]:\n",
    "\n",
    "                    try:\n",
    "                        url = media['media'][0]['media_url_https']\n",
    "                        porno_score = nsfw(url,sess)\n",
    "                        if porno_score > 0.8:\n",
    "                            porn += 1\n",
    "                        tot+=1\n",
    "\n",
    "                        #print(url, porno_score)\n",
    "\n",
    "                    except:\n",
    "                        print('exception')\n",
    "\n",
    "                if tot > 0:\n",
    "                    nudity = porn/tot\n",
    "                else:\n",
    "                    nudity = 0\n",
    "            except:\n",
    "                nudity = 0\n",
    "\n",
    "            if user.default_profile_image == False:\n",
    "                try:\n",
    "                    profile = user.profile_image_url_https.replace('normal', '400x400')\n",
    "                    profile_nudity = nsfw(profile, sess)\n",
    "                except:\n",
    "                    profile_nudity = 0\n",
    "            else:\n",
    "                profile_nudity = 0\n",
    "            return profile_nudity, nudity\n",
    "\n",
    "\n",
    "\n",
    "    nsfw_profile, nsfw_avg = nsfw_detection(user.id)\n",
    "    nsfw_data = pd.DataFrame({'nsfw_profile': nsfw_profile, 'nsfw_avg': nsfw_avg},index=[0])\n",
    "    \n",
    "    if user.default_profile_image == False:\n",
    "        os.unlink('local-filename.jpg')\n",
    "\n",
    "    # collect descriptive features\n",
    "\n",
    "    def describe_tweets(tweets):\n",
    "\n",
    "        ret_perc, media_perc, url_perc, quote_perc = tweet_perc(tweets)\n",
    "\n",
    "        avg_len, avg_ret, avg_fav, avg_hash = tweet_desc(tweets, 'avg')\n",
    "        max_len, max_ret, max_fav, max_hash = tweet_desc(tweets, 'max')\n",
    "        min_len, min_ret, min_fav, min_hash = tweet_desc(tweets, 'min')\n",
    "\n",
    "        freq = tweet_freq(tweets)\n",
    "\n",
    "        frame = np.array([avg_len, max_len, min_len, avg_ret, max_ret, min_ret, avg_fav, max_fav, min_fav, avg_hash, max_hash, min_hash, freq, ret_perc, media_perc, url_perc, quote_perc])\n",
    "\n",
    "        desc_features = pd.DataFrame({'avg_len': avg_len, 'max_len': max_len, 'min_len': min_len, 'avg_ret': avg_ret, 'max_ret': max_ret, 'min_ret': min_ret, 'avg_fav': avg_fav, 'max_fav': max_fav, 'min_fav': min_fav, 'avg_hash' : avg_hash, 'max_hash' : max_hash, 'min_hash' : max_hash,'freq': freq, 'ret_perc': ret_perc, 'media_perc': media_perc, 'url_perc': url_perc, 'quote_perc': quote_perc}, index=[0])\n",
    "\n",
    "        return desc_features\n",
    "\n",
    "\n",
    "    def tweet_perc(tweets):\n",
    "\n",
    "        try:\n",
    "            ret_perc = np.invert(tweets.retweeted_status.isnull()).sum()/len(tweets)\n",
    "        except:\n",
    "            ret_perc = 0\n",
    "        try:\n",
    "            media_perc = np.invert(tweets.extended_entities.isnull()).sum()/len(tweets)\n",
    "        except:\n",
    "            media_perc = 0\n",
    "        try:\n",
    "            url_perc = np.invert(tweets.url.isnull()).sum()/len(tweets)\n",
    "        except:\n",
    "            url_perc = 0\n",
    "        try:\n",
    "            quote_perc = tweets.is_quote_status.sum()/len(tweets)\n",
    "        except:\n",
    "            quote_perc = 0\n",
    "\n",
    "        return ret_perc, media_perc, url_perc, quote_perc\n",
    "\n",
    "    def hashtag_count(tweets):\n",
    "\n",
    "        occurrences = []\n",
    "        for tweet in tweets:\n",
    "            occurrences.append(tweet.count('#'))\n",
    "\n",
    "        return occurrences\n",
    "\n",
    "    def tweet_desc(tweets, metric):\n",
    "\n",
    "        tweets_lenght = tweets['full_text'].apply(lambda x: len(x))\n",
    "\n",
    "        if metric == 'avg':\n",
    "            ret = np.mean(tweets.retweet_count)\n",
    "            lenght = np.mean(tweets_lenght)\n",
    "            fav = np.mean(tweets.favorite_count)\n",
    "            hashtag = np.mean(hashtag_count(tweets['full_text']))\n",
    "        elif metric == 'max':\n",
    "            ret = max(tweets.retweet_count)\n",
    "            lenght = max(tweets_lenght)\n",
    "            fav = max(tweets.favorite_count)\n",
    "            hashtag = max(hashtag_count(tweets['full_text']))\n",
    "        elif metric == 'min':\n",
    "            ret = min(tweets.retweet_count)\n",
    "            lenght = min(tweets_lenght)\n",
    "            fav = min(tweets.favorite_count)\n",
    "            hashtag = min(hashtag_count(tweets['full_text']))\n",
    "\n",
    "        return lenght, ret, fav, hashtag\n",
    "\n",
    "    def tweet_freq(tweets):\n",
    "\n",
    "        dates = list(tweets.created_at)\n",
    "\n",
    "        last = dates[0]\n",
    "        d = last[8:10]\n",
    "        m = last[4:7]\n",
    "        y = last[-4:]\n",
    "        date = d + ' ' + m + ' ' + y\n",
    "        last = datetime.strptime(date, '%d %b %Y')\n",
    "\n",
    "        first = dates[-1]\n",
    "        d = first[8:10]\n",
    "        m = first[4:7]\n",
    "        y = first[-4:]\n",
    "        date = d + ' ' + m + ' ' + y\n",
    "        first = datetime.strptime(date, '%d %b %Y')\n",
    "\n",
    "        delta = (last - first).days + 1\n",
    "        freq = len(tweets)/delta\n",
    "\n",
    "        return freq\n",
    "\n",
    "    def describe(tweets_df):\n",
    "\n",
    "        tweets = tweets_df\n",
    "\n",
    "        if len(tweets) > 0:\n",
    "            # sum all the scores of each category\n",
    "            features = describe_tweets(tweets)\n",
    "        else:\n",
    "            features = pd.DataFrame({'avg_len': 0, 'max_len': 0, 'min_len': 0,'avg_ret': 0, 'max_ret': 0, 'min_ret': 0, 'avg_fav': 0, 'max_fav': 0, 'min_fav': 0, 'avg_hash': 0, 'max_hash': 0, 'min_hash': 0, 'freq': 0, 'ret_perc': 0, 'media_perc': 0, 'url_perc': 0, 'quote_perc':0}, index=[0])\n",
    "\n",
    "        # return the average scores of each user\n",
    "        return features\n",
    "\n",
    "    features = describe(tweets)\n",
    "\n",
    "    # create dataset with all features\n",
    "\n",
    "    full = pd.concat([user_features, intradistances, scores, features, nsfw_data], axis=1)\n",
    "\n",
    "\n",
    "    def age(x):\n",
    "        x = str(x)\n",
    "        if x[0] == '2':\n",
    "            return 2018 - int(x[:4])\n",
    "        else:\n",
    "            return 2018 - int(x[-4:])\n",
    "\n",
    "    full = full.drop(columns=['contributors_enabled', 'follow_request_sent', 'following', 'profile_background_image_url', 'profile_background_image_url_https', 'profile_image_url', 'profile_image_url_https', 'time_zone', 'utc_offset'])\n",
    "    full = full.drop(columns=['default_profile_image','is_translator', 'geo_enabled', 'location', 'notifications', 'profile_background_tile', 'protected'])\n",
    "    full['default_profile'] = full['default_profile'].apply(lambda x: int(x))\n",
    "    full['description'] = full['description'].fillna('')\n",
    "    full['description_len'] = full['description'].apply(lambda x: len(x))\n",
    "    full = full.drop(columns=['description'])\n",
    "    full['name'] = full['name'].fillna('')\n",
    "    full['name_len'] = full['name'].apply(lambda x: len(x))\n",
    "    full = full.drop(columns=['name'])\n",
    "    full['screen_name'] = full['screen_name'].fillna('')\n",
    "    full['screen_name_len'] = full['screen_name'].apply(lambda x: len(x))\n",
    "    full = full.drop(columns=['screen_name'])\n",
    "    full = full.drop(columns=['lang'])\n",
    "    full['age'] = full['created_at'].apply(lambda x: age(x))\n",
    "    full = full.drop(columns=['created_at'])\n",
    "    full = full.drop(columns=['id'])\n",
    "    full['profile_use_background_image'] = full['profile_use_background_image'].apply(lambda x: int(x))\n",
    "    full['url'] = (full['url'].notnull()).astype(int)\n",
    "\n",
    "    print(full.iloc[0])\n",
    "\n",
    "    # BoN classification\n",
    "    data = full[['avg_fav', 'avg_hash', 'avg_len', 'avg_ret', 'bot_words_score',\n",
    "       'default_profile', 'description_len', 'favourites_count',\n",
    "       'followers_count', 'freq', 'friends_count', 'gen_words_score',\n",
    "       'listed_count', 'max_fav', 'max_hash', 'max_len', 'max_ret',\n",
    "       'media_perc', 'min_fav', 'min_hash', 'min_len', 'min_ret', 'name_len',\n",
    "       'nsfw_profile', 'profile_use_background_image', 'quote_perc',\n",
    "       'ret_perc', 'screen_name_len', 'statuses_count', 'tweet_intradistance',\n",
    "       'url', 'url_entropy', 'url_perc', 'verified']]\n",
    "\n",
    "    bon_pred = bon.predict_proba(data)\n",
    "    print('### BINARY ###')\n",
    "    print(bon_pred)\n",
    "\n",
    "\n",
    "    # RF classification\n",
    "    data = full[['NSFW_words_score','news_spreaders_words_score','spam_bots_words_score','fake_followers_words_score',\n",
    "         'avg_fav','avg_hash','avg_len','avg_ret','default_profile','favourites_count','followers_count',\n",
    "         'freq','friends_count','listed_count','max_fav','max_hash','max_len','max_ret','media_perc','min_hash',\n",
    "         'min_len','min_ret','nsfw_avg','nsfw_profile','profile_use_background_image','quote_perc','ret_perc',\n",
    "         'statuses_count','tweet_intradistance','url','url_perc','description_len','name_len','screen_name_len',\n",
    "         'age','url_entropy','min_fav']]\n",
    "\n",
    "    data = data.reindex(sorted(data.columns), axis=1)\n",
    "    rf_scores = rf.predict_proba(data)\n",
    "    rf_prob = pd.DataFrame(rf_scores, columns=['rf_0', 'rf_1', 'rf_2', 'rf_3'])\n",
    "    print('### MULTICLASS ###')\n",
    "    print('-- RF: --')\n",
    "    print(rf_prob)\n",
    "\n",
    "\n",
    "    ## KNN classification\n",
    "    data = full[['default_profile', 'favourites_count', 'followers_count', 'friends_count', 'listed_count','profile_use_background_image',\n",
    "       'statuses_count', 'url', 'description_len', 'name_len', 'screen_name_len', 'age', 'nsfw_profile']]\n",
    "\n",
    "    data = np.array(data) * knn_weights\n",
    "    knn_scores = knn.predict_proba(data)\n",
    "    knn_prob = pd.DataFrame(knn_scores, columns=['knn_0', 'knn_1', 'knn_2', 'knn_3'])\n",
    "    print('-- KNN: --')\n",
    "    print(knn_prob)\n",
    "\n",
    "\n",
    "\n",
    "    # NB classification\n",
    "    if len(tweets) != 0:\n",
    "        tweets = tweets['full_text']\n",
    "\n",
    "        # define preprocess functions\n",
    "\n",
    "        def remove_rt(x):\n",
    "            if 'RT @' in x:\n",
    "                try:\n",
    "                    return x[x.find(':')+2:]\n",
    "                except:\n",
    "                    return x\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "        def remove_stop(x):\n",
    "            return [word for word in x.split() if word not in stop_words]\n",
    "\n",
    "\n",
    "        # preprocess tweets\n",
    "\n",
    "        tweets = tweets.apply(lambda x: remove_rt(x))\n",
    "        tweets = tweets.apply(lambda x: re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x))\n",
    "        tweets = tweets.apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "        tweets = tweets.apply(lambda x: x.lower())\n",
    "        tweets = tweets.apply(lambda x: remove_stop(x))\n",
    "        tweets = tweets.astype(str)\n",
    "        tweets = tweets[tweets!='[]']\n",
    "\n",
    "\n",
    "        # perform predictions over tweets\n",
    "\n",
    "        pred = nb.predict_proba(tweets)\n",
    "\n",
    "\n",
    "        # return average of NB predictions\n",
    "\n",
    "        nb_scores = np.mean(pred, axis=0)\n",
    "\n",
    "    else:\n",
    "        nb_scores = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "    nb_scores = np.array(nb_scores)\n",
    "    nb_scores = np.expand_dims(nb_scores, axis=0)\n",
    "    nb_prob = pd.DataFrame(nb_scores, columns=['nb_0', 'nb_1', 'nb_2', 'nb_3'])\n",
    "    print('-- NB: --')\n",
    "    print(nb_prob)\n",
    "\n",
    "\n",
    "    # merge predictions\n",
    "    prob = pd.concat([knn_prob, nb_prob, rf_prob], axis=1)\n",
    "\n",
    "\n",
    "    # LR classification\n",
    "\n",
    "    mc = lr.predict_proba(prob)\n",
    "\n",
    "    # compute final prediction\n",
    "    final = []\n",
    "    final.append(bon_pred[0][1] * mc[0][0])\n",
    "    final.append(bon_pred[0][1] * mc[0][1])\n",
    "    final.append(bon_pred[0][1] * mc[0][2])\n",
    "    final.append(bon_pred[0][1] * mc[0][3])\n",
    "    final.append(bon_pred[0][0])\n",
    "\n",
    "\n",
    "    #except Exception as e:\n",
    "     #   classification = str(e)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genuine --- 3565 / 8767   ---   errors: 670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:215: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "i, errors = 0, 0\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "users = users[7500:]\n",
    "for user in users:\n",
    "    i += 1\n",
    "    try:\n",
    "        tg = classify(user)\n",
    "        targets.append(tg)\n",
    "    except:\n",
    "        errors += 1\n",
    "    clear_output()\n",
    "    print(str(i) + \" / \" + str(len(users)) + '   ---   errors: ' + str(errors))\n",
    "    if i % 500 == 0:\n",
    "        time.sleep(300)\n",
    "    if i % 50 == 0:\n",
    "        pickle.dump(targets, open('../classification/global_class2.targets','wb'))  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
